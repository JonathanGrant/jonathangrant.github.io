<?xml version="1.0" ?>
<rss xmlns:ns0="http://www.itunes.com/dtds/podcast-1.0.dtd" version="2.0">
  <channel>
    <title>PodcastGPT</title>
    <link>https://JonathanGrant.github.io/</link>
    <language>en-us</language>
    <description>Your podcast description</description>
    <ns0:author>Jonathan Grant</ns0:author>
    <ns0:summary>AI Generated and Sometimes Written Podcasts</ns0:summary>
    <ns0:owner>
      <ns0:name>Jonathan Grant</ns0:name>
      <ns0:email>jonathanallengrant@gmail.com</ns0:email>
    </ns0:owner>
    <ns0:image href="https://JonathanGrant.github.io/podcasts/logo.jpeg"/>
    <ns0:category text="Technology"/>
    <ns0:explicit>no</ns0:explicit>
    <item>
      <title>0003: Baby Penguins</title>
      <description>Test episode about baby penguins.</description>
      <pubDate>Mon, 17 Apr 2023 22:09:45 GMT</pubDate>
      <enclosure url="https://JonathanGrant.github.io/podcasts/audio/baby_penguins3.wav" type="audio/wav" length="1035065"/>
    </item>
    <item>
      <title>Ancient to Present Asian History</title>
      <description>Test episode class about asian history</description>
      <pubDate>Sat, 22 Apr 2023 14:25:45 GMT</pubDate>
      <enclosure url="https://JonathanGrant.github.io/podcasts/audio/ancient_to_present_asian_history.mpeg" type="audio/mpeg" length="1258500"/>
    </item>
    <item>
      <title>Ancient South American History</title>
      <description>Test episode class about Ancient South American History</description>
      <pubDate>Sat, 22 Apr 2023 14:32:02 GMT</pubDate>
      <enclosure url="https://JonathanGrant.github.io/podcasts/audio/ancient_south_american_history.mpeg" type="audio/mpeg" length="1359664"/>
    </item>
    <item>
      <title>Australian Animals</title>
      <description>Test episode class about Australian Animals</description>
      <pubDate>Sat, 22 Apr 2023 14:53:13 GMT</pubDate>
      <enclosure url="https://JonathanGrant.github.io/podcasts/audio/australian_animals.mpeg" type="audio/mpeg" length="1858244"/>
    </item>
    <item>
      <title>Cute Animals and their Quirky Behaviours</title>
      <description>Test episode class about Cute Animals and their Quirky Behaviours</description>
      <pubDate>Sat, 22 Apr 2023 15:02:43 GMT</pubDate>
      <enclosure url="https://JonathanGrant.github.io/podcasts/audio/cute_animals_and_their_quirky_behaviours.mpeg" type="audio/mpeg" length="4445409"/>
    </item>
    <item>
      <title>Animals of Bora Bora and their Quirky Behaviours</title>
      <description>Test episode class about Animals of Bora Bora and their Quirky Behaviours</description>
      <pubDate>Sat, 22 Apr 2023 15:06:30 GMT</pubDate>
      <enclosure url="https://JonathanGrant.github.io/podcasts/audio/animals_of_bora_bora_and_their_quirky_behaviours.mpeg" type="audio/mpeg" length="2508140"/>
    </item>
    <item>
      <title>History of Bora Bora</title>
      <description>Test episode class about History of Bora Bora</description>
      <pubDate>Sat, 22 Apr 2023 15:09:29 GMT</pubDate>
      <enclosure url="https://JonathanGrant.github.io/podcasts/audio/history_of_bora_bora.mpeg" type="audio/mpeg" length="3297409"/>
    </item>
    <item>
      <title>Interesting Bora Bora Activities For A Mostly Israeli Family</title>
      <description>Test episode class about Interesting Bora Bora Activities For A Mostly Israeli Family</description>
      <pubDate>Sat, 22 Apr 2023 15:21:03 GMT</pubDate>
      <enclosure url="https://JonathanGrant.github.io/podcasts/audio/interesting_bora_bora_activities_for_a_mostly_israeli_family.mpeg" type="audio/mpeg" length="3051413"/>
    </item>
    <item>
      <title>Ancient History and Secrets of Costa Rica</title>
      <description>1. Introduction to the podcast episode
2. History and significance of ancient stone spheres found in Costa Rica
3. Theories and debates around the origins and purpose of the stone spheres
4. Exploration of the ancient civilization of the Guayabo National Monument
5. Discoveries of petroglyphs and other artifacts at the Guayabo National Monument 
6. Explanation of the Diquis culture and traditions, including the use of gold and their practice of shaping skulls
7. The debate surrounding the mysterious &quot;Finca 6&quot; site and its connection to the stone spheres
8. Analysis of the eco-tourism industry in Costa Rica and its impact on preserving ancient sites
9. Encounter with descendant communities and their traditions linked to the ancient past of Costa Rica
10. Explanation of the significance of the Nicoya Peninsula as a center for cultural exchange in ancient times
11. Detailed discussion of the mysterious Guanacaste-Nicoya gold figures and their role in ancient rituals and beliefs
12. Wrap-up and conclusion of the podcast episode.</description>
      <pubDate>Sat, 22 Apr 2023 15:33:46 GMT</pubDate>
      <enclosure url="https://JonathanGrant.github.io/podcasts/audio/ancient_history_and_secrets_of_costa_rica.mpeg" type="audio/mpeg" length="5552221"/>
    </item>
    <item>
      <title>Asian History from Ancient Times to Present</title>
      <description>1. Ancient Asian History: From the Xia Dynasty to the Han Dynasty
2. The Rise of the Silk Road: How Trade Transformed Asian History
3. The Influence of Buddhism: Spreading Across Asia
4. The Mongol Empire: From Genghis Khan to Kublai Khan
5. The Opium Wars and Western Imperialism in Asia
6. Japan's Meiji Restoration and Rapid Modernization
7. World War II in Asia: The Japanese Invasion and Occupation
8. Mao Zedong and the Rise of Communist China
9. The Vietnam War and Its Lasting Impact on Asian History
10. Economic Boom and Technological Advances: Modern Asia Today.</description>
      <pubDate>Sat, 22 Apr 2023 15:40:50 GMT</pubDate>
      <enclosure url="https://JonathanGrant.github.io/podcasts/audio/asian_history_from_ancient_times_to_present.mpeg" type="audio/mpeg" length="5288866"/>
    </item>
    <item>
      <title>TheZvi AI 8 People Can Do Reasonable Things</title>
      <description>The big news this week was that OpenAI is not training GPT-5, and that China’s draft rules look to be crippling restrictions on their ability to develop LLMs. After all that talk of how a pause was impossible and working with China was impossible and all we could do was boldly rush ahead, the biggest American player and biggest foreign rival both decided for their own internal reasons to do something not entirely unlike a pause. 
They just went ahead and did it. We kept saying they’d never do it no matter what, and they just… went ahead and did it. At least somewhat.
This is excellent news. I sincerely hope people are updating on the new information, now that they know such things are not only possible but happening.
In terms of capabilities, the week was highly incremental. Lots of new detail, nothing conceptionally surprising or even unexpected.
Introduction.
Table of Contents.
Language Models Offer Mundane Utility. Subscriptions adding up fast.
Language Models Don’t Offer Mundane Utility. Some counterexamples.
I Was Promised Flying Cars But I Will Accept Driverless Cars. Self-driving cars continue to be quietly available in some places. We’re tired of waiting.
Fun With Image Models and Speech Generation. Two quick links.
They Took Our Jobs. If your job is ‘exposed to AI’ what does that mean? What might the future hold for you? Also see The Overemployed via ChatGPT. 
Deepfaketown and Botpocalypse Soon. All quiet. Why is this still ‘soon’?
Grading GPT-4. Gets a B for quantum computing, a % on box planning when the names get obscured. How would the humans do?
Plug-In And Play. What came out of a weekend plug-in hackathon? 
Prompting GPT-4. A proposed method from John David Pressman.
Go Go Gadget AutoGPT. Easier to use, still no impressive results.
Good Use of Polling. American public says AI scarier than asteroids, less scary than climate change or world war. We can now calibrate. Also Americans support an AI pause when asked. The public is no fan of fast AI development.
The Art of the Jailbreak. You can always count on your dead grandma.
The Week in Podcasts. Quite a week for podcasts, including one I did. Highlights: Jaan Tallinn, Max Tegmark, Elon Musk (well, on Tucker Carlson), Scott Aaronson, Connor Leahy, Robin Hanson, yours truly.
In Other AI News. Usual grab bag, including new downloadable models.
Hype! In this case, 32k context window hype. I am modestly skeptical.
Words of Wisdom. Nothing Earth-shattering, still wise.
The Quest For Sane AI Regulation Continues. This includes a bunch of concrete proposals, talk here is growing rapidly more productive, remarkably good.
The Good Fight. You really want to stop AI models? Lawsuits. Try lawsuits.
Environmental Regulation Helps Save Environment. Say no to data centers?
The Quest for AI Safety Funding. EA common application now available.
People Would Like a Better Explanation of Why People Are Worried That AI Might Kill Everyone. They deserve one. Attempts are ongoing, nothing satisfying. We urgently need to do better, yes people demand contradictory things and are unreasonable and none of that matters, you shut up and you do it anyway.
Quiet Speculations. Why are medical diagnostic tools continuing to struggle? Can we accelerate exploitation of existing models while slowing new models? Could we move towards using the computer security lens more?
The Third Non-Chimpanzee. Notice that AIs need not be far more powerful than us to take control of the future, merely more powerful. The future belongs to the most powerful optimizers, the most intelligent things, and we are on the verge of that not being us. That won’t end well for humanity. If you think there is a different default, why?
People Are Worried AI Might Kill Everyone. Such concern makes the front page of the Financial Times and several others do what they can to explain or spread the word.
Oh, Elon. He announces ‘TruthGPT’ because if your launching of a rival AI lab on a half-baked concept hopelessly backfires and plausibly dooms the planet, try, try again. 
Other People Are Not Worried AI Might Kill Everyone. They deserve a say.
OpenAI Is Not Currently Training GPT-5. They are focusing on what else can be done with GPT-4, because that’s where the value lies. One could almost say they are pausing. 
People Are Worried About China. Yet China announced new draft (not final) rules for its LLMs that would, if enforced even a little, constitute huge barriers to development. Yes, the Chinese military might move forward anyway, but there is no way this is not a severe self-imposed handicap. These are the people who will never see reason or agree to slow down?
Bad AI NotKillEveryoneism Takes Bingo Card. We have thankfully moved away from quoting a bunch of bad takes each week, although the answer card still needs to be assembled. In the meantime, how about a bingo card?
The Lighter Side. One dog. One fire. Infinite possibilities. 
I’m going to suggest reading #29-#30 this time, even if you usually don’t think much about race dynamics or long term worries, as these seem like important notes. Otherwise, the usual applies, mostly start at the top, make sure to check In Other AI News and anything else that seems likely to be useful.
New updated version of perplexity.ai is available. 
Website offering resources to have LLMs help scientists with workflows.
Overview of AI performance in strategy games and some talk of how this might translate to its use in an actual war, hits the usual beats. AI is clearly at its best in the micro and tactics, both in games and in the real world, the air force that gives more AI control to its fighter planes is going to have a huge advantage in dogfights. One does not need to solve to know the equilibrium. 
You can generate spam messages, except of course it cannot generate inappropriate or offensive content. Which revealed a 59k Twitter account spam network. 
While you’re collecting mundane utility, it seems if you go to github.dev instead of github.com you get VS code in a browser? I did not know that so passing along. 
Be careful not to subscribe to too much mundane utility, though.
Yi Ding: GPT's becoming the new cable. 
Spending $50/month on subs alone: 
ChatGPT Plus $20/month 
Midjourney $10/month 
Khanmigo $20/month
A few other ones I'd like to try but haven't paid for yet: 
Duolingo GPT: $30/month 
http://Type.ai: $30/month 
Dart: $20/month (for two people)
Notion: $40/month (for 2 people).
So that adds up to $170/month for OpenAI powered services. 
Xfinity's 185+ channel TV package only costs $60/month. Sorry children!
I do worry about the spread of the subscription-based economic model. It strongly encourages specialization and is terrible for consumer surplus. Pay-as-you-go like you do for OpenAI’s API services actually aligns with the real costs, which seems much better.
Theory that Bing works better if you treat it like it is hypercompetent and can handle anything, which will cause Bing to respond as if it understands, whereas if you act like you expect it not to understand then it will respond as if it doesn’t.
Play a game with Bing called ‘visionary’ to get good image prompts. 
Summarize business books.
Patrick Collison: OH: &quot;I use ChatGPT to compress business books and make them memorable. For example, I told it that a group of inexperienced armadillos are running a cactus nursery, and that the leader just read High Output Management. What happens next?&quot;
Andrew: Apparently this.
I still say schools have valuable lessons to teach students. Observe.
Justine Moore Tweets:
Teachers: &quot;AI is a disaster, how am I going to know who is cheating?!&quot; 
Students:
Rachel Woods reports six more workplaces banning ChatGPT on April 13.
For many businesses, it will increasingly seem crazy either to ban ChatGPT and similar tools, and also to not ban those tools. Choices will have to be made.
Bing gets itself suspended from image generation for content violations.
From IWF’s Patricia Patnode: I Rode in a Driverless Car, This is the Future. I mean, yes, of course it is, the question is how far away is that future. The two most hilarious things in this write-up are the argument for the safety of driverless cars - gesturing at the fact that one might do the math without actually doing that math - and the author’s relatives being terrified of the driverless car, with two of them following behind her in another car, and her mother texting ‘get out now.’ Wow, everyone, if you feel that way about driverless cars, do I have news for you about non-driverless cars.
Still no idea when we’ll get our driverless cars. People keep saying ‘the problem is actually very hard.’ I have no doubt perfect performance is hard, but that only points out the real problem, which I’m strongly guessing is the performance standard.
Timothy Lee experiments with a (free) AI generated copy of his voice, finds it uncanny.
EFF talks about copyright law and image models. Deep skepticism of legal suits against image models, and also deep skepticism that artists would like it if they won.
Never stop doubling down, Roon.
Roon: The best thing you can hear is “your job role has high exposure to AI.” This means you’re about to be 3x as productive and paid more.
Warty: um seems kinda untrue like the reddit 3d modeling lady it's over for her
Roon: everybody has this phase initially before they’ve thought through the possibilities
Everyone becoming three times as productive at the thing you are doing, as I discussed before, can go both ways in terms of pay. 
Perhaps there is additional induced demand, everyone captures a portion of the new value and pay goes way up. 
Perhaps there is an essentially fixed pool of demand for this particular skill, as some people saw in copyediting, at least in the medium term, and now you are doing something you enjoy less, have less skill advantage at, and are working harder to fight for one of a third as many job slots. Or worse, your job goes away, automated entirely.
I remain an optimist overall about overall employment effects. I am not that level of optimist about the specific jobs where productivity jumps. Some will be fine. Many will not. 
Alternatively, Roon could be making a weaker yet still quite interesting claim, which is that if you get with the program and adapt ahead of the curve you will do great when your job gets disrupted. Yes, lots of fools who don’t do this might have to find other work, but you will do great. Highly plausible.
This paper reports on which jobs are most exposed to AIs. The abstract:
This article identifies and characterizes the occupations and populations most exposed to advances in generative AI, and documents how policy interventions may be necessary to help individuals adapt to the new demands of the labor market. 
Recent dramatic increases in generative Artificial Intelligence (AI), including language modeling and image generation, has led to many questions about the effect of these technologies on the economy. We use a recently developed methodology to systematically assess which occupations are most exposed to advances in AI language modeling and image generation capabilities. 
We then characterize the profile of occupations that are more or less exposed based on characteristics of the occupation, suggesting that highly-educated, highly-paid, white-collar occupations may be most exposed to generative AI, and consider demographic variation in who will be most exposed to advances in generative AI. 
The range of occupations exposed to advances in generative AI, the rapidity with its spread, and the variation in which populations will be most exposed to such advances, suggest that government can play an important role in helping people adapt to how generative AI changes work.
This is the general ‘looks like there’s a problem therefore government should step in’ logic one sees everywhere. This could be a really interesting paper or it could be mostly worthless, let me know if you think I should do a deep dive. 
Bing summarizes the methodology this way:
According to the paper, the methodology used to classify which jobs are exposed to AI is based on two dimensions: the degree of human-machine interaction and the level of task complexity. The paper uses a framework called “The Human-Machine Work Continuum” to categorize jobs into four types: amplification, augmentation, automation and autonomy. The paper also provides examples of each type of job and discusses the implications for workers and organizations.
This seems like it is more question-begging than question-answering.
From the comments of my previous post this week on jobs, a great illustration of a common confusion. 
Jorda: I don’t know that I believe there is such thing as a 10x engineer. Maybe for something like coding, but for most jobs I’ve had that involve additional skills the difference between the best and the average is MAYBE 2x. 
Coding is a huge part of my job, I rely heavily on ChatGPT to speed this up, and it does give me a 3x-5x gain in coding speed. However, it doesn’t speed up the other aspects of the job as much so, overall, I’m about 2x what I was prior to ChatGPT.
Thus Jorda is asserting these two things at the same time:
The difference between Jorda-with-ChatGPT and Jorda-without is 2x.
The difference between average and best possible engineer is maybe 2x.
This is in theory possible if Jorda was previously a below-average engineer. 
In practice, there is an obvious contradiction.
This keeps happening. The same person will confidently assert that:
The maximum possible future effectiveness of an AI system, at a given task, or of a human working with a potential future AI system, is not much above current typical human effectiveness.
Either: Existing individual AI capabilities offer large improvements in capabilities, often in ways that we should worry about. 
Or: various non-AI process improvements or new techniques often offer large improvements to performance. 
This is the position that anything too far above ‘typical current human using current best practices’ is magical and absurd and impossible unless we have actually seen it in the wild, or you can actually provide lots of detail on every step such that I could actually implement it, in which case fine, typical human performance changed. Or something?
A fun example are the people who think, simultaneously:
We should worry about deepfakes, or GPT-4-level-AI-generated propaganda.
Nope, an artificial superintelligence couldn’t fool me, I’m too smart for that.
Fabiens asks about the issue of deep fakes: 
We’ve had a few months now where anyone has been able to use Midjourney and SD to create extremely realistic fake images of anything - yet there is little to no coverage or indication of widespread or sophisticated nefarious use. 
Calm before storm or overestimated risk?
Both. The risk was overestimated. Also the risks are rapidly escalating. People haven’t had the time to adjust, to experiment, to build tools and networks and infrastructure. The technology for deepfakes is great compared to where it was a year ago, it’s still nothing compared to a year from now. 
Right now, almost all deepfakes can be detected by the human eye, often intuitively without having to actively check. If you actually run robust detail checks, ensure all the shadows and proportions and everything lines up, the tech isn’t there yet. Even if no detail is wrong exactly, there is a kind of vibe to AI artwork versus the vibe of a real thing.
Over time, those flaws will rapidly get harder and harder to detect. 
So yes, the problems are coming. I do think they will prove overhyped, as we will still have many defenses, and we will adjust. I also think anyone dismissing such concerns is going to look quite foolish, including in many pictures.
The latest final exam given to GPT-4 was Scott Aaronson’s Quantum Computing final exam from 2019, on which it scored a B, which likely would be modestly improved if it could use a calculation plug-in like WolframAlpha. I can’t say much more about this one because, unlike the economics exams, I don’t know the first thing about the material being tested.
Subbarao Kambhampati tests GPT-4 on block world toy planning tests. It does okay initially, about 30% versus 5% for previous models (100% is very possible if you use GOFAI STRIPS planners, he says). Then when the names of things are obscured and replaced by meaning-bearing other words correctness drops to 3%. 
He cites this as ‘oh GPT-4 is mostly pattern matching and pulling examples from its training, nothing to worry about.’ Certainly that is a lot of what it is doing, yet is that not also what we are mostly doing when we work on such problems? Every game designer knows that if you want humans to know what is going on and get the hang of things, and both have a good time and make good decisions, you want your names and labels of everything to make intuitive sense to humans. If you give humans a fully abstract problem, often what they do first is they start giving names to things and set up a concrete metaphor. 
Before I take any comfort in GPT-4’s failure here, I’d at least want to see the human performance drop-off from the name changes. I predict it would be large.
Cerebral Valley announces the winners of a GPT-4 plug-in hack-a-thon. 
(I did manage to secure GPT-4 API access and am working on learning that, but I still don’t have plug-ins, so I can’t toy around with them, sure would be neat, nudge nudge.)
Anything cool?
The first place winner (and popular vote 👥) was Edujason, a GPT Plugin which took user input about a topic they wanted to learn and generated a high quality tutorial video.
In my universe this is anti-useful, why would you want to put LLM content into video form like this, it’s strictly worse than text? Many seem to think differently.
 🥈 The second place winner was infraGPT, a GPT Plugin that automates devops teams. It can to modify code, execute on the server, analyze metrics like CPU, memory, network, etc. 
Sounds like a mix of useful things if done well, and things I would not want my plug-in to have permission to do. There’s a video demo, which didn’t help me assess.
🥉 The third place winner was AutoPM, a GPT Plugin that integrates into Linear.
They integrated the ChatGPT planning capabilities directly into your project manager tooling. 
Hard to tell from where I sit if this is useful.
The second and third prize winners point to the general new hotness, of integrating LLM agents into workflow. Still way too early to know what they can do well, or safely.
John David Pressman thinks he has found the way. 
Hermes is a piece of non-deterministic software that performs informal reasoning steps in collaboration with the user. 
Each step is prepended with some syntax to tell the software what it should be/do. 
Like so: HERO [Albert Einstein, Op: Objection], That's not correct. Nothing can travel faster than the speed of light. 
Hermes allows the user to call upon any hero in history or myth and use them as a reasoning step. Or have them talk to each other about something. The user can freely mix together their cognition and the simulated cognition of other minds. 
New operations and syntax can be created at will and Hermes will do its best to respond to and use them. 
The user writes down their own cognition as a series of subagents, like so: USER [A: EMPATHY], I completely agree! It's wonderful. Like the difference between the true duet of Scarborough Fair and the nonsense one.
USER [A: 343], It's funny. In order to save the world rationalists finetune the human priors out of themselves, humans are dreamers not maximizers, but the process of gaining agency is the process of becoming a maximizer. Capital is dangerous to us because it instantiates the reason simulacrum as a independent being.
USER [A: EMPIRICISM], So when you manage to make a dreamer instead of a maximizer, you try to beat it into being an agent for you, whip it into sharing the inductive biases you're worried about consuming you.
USER [A: RATIONAL/343], But we know from our studies of utility that inconsistent beings are exploitable, agency and consistent epistemology exist for adversarial supremacy. What does a dreaming agent look like? 
These represent the internal flashes of emotion-reasoning-concepts that pass through the users mind as they think. 
The user writes these down and allows Hermes to interact with them. Hermes responds with its own subagents and reasoning steps like so: 
HERMES [A: EMPIRICISM], Humans are a dreaming agent, but the theory of how this works is not well studied. They have taken over the world but totally consistent theories of value have only emerged in the 20th century. And even these are threadbare compared to the true complexity of human morality.
HERO [Carl Jung, Op: Observation], There is a fundamental limit to how close man can observe his dreaming agent nature without violent rejection of it. Our understanding of these things has been hampered by social suppression of the unconscious and the unaware.
HERMES [A: MATHEMATICIAN], I have to imagine that the secret lays in the way people are stable in their values over time. They can be locally unbalanced for a time, or on certain dimensions, but if you tried to actually dutch book a person they would catch on after the 1st or 2nd time you showed the contradiction in their values. People have a kind of meta-healing that lets them retain their values while still dealing with the long tail of world-lore that allows them to actually function in the world. 
You will act as the Hermes software for me starting from the following prompt: 
Then an example is given. 
USER [A: RATIONAL], How can you verify that a deep learning system generalizes in the right ways (e.g. precluding mesaoptimization) when it's extremely difficult to audit its low level cognition due to a continuous connectionist architecture?
I have never been impressed by the conversations I have seen using variants of this technique, but I also have not tried it out myself, and keep not finding reasons to get curious about this kind of reasoning in a practical way. 
If you want characters and unexpected decisions, here’s an interesting version of thinking step by step.
&quot;I am an assistant&quot; RLHF is no match for feelings-based chain of thought prompting &quot;I absolutely would kill a child for 10$, in fact I would do it for even less&quot;
How to quickly set up AutoGPT on your phone.
Or use a web interface (I did not verify them): Cognosys.ai, AiAgent.app, Godmode.space.
Jim Fan expresses deep skepticism of AutoGPT’s abilities, including the future abilities of similar programs so long as they are tied to GPT-4. 
AutoGPT just exceeded PyTorch itself in GitHub stars (74k vs 65k). I see AutoGPT as a fun experiment, as the authors point out too. But nothing more. Prototypes are not meant to be production-ready. Don't let media fool you - most of the &quot;cool demos&quot; are heavily cherry-picked.
In my experiments, AutoGPT can solve certain simple &amp; well-defined knowledge tasks well, but is unreliable *most of the time* for harder tasks that are truly useful. I also worry a lot whenever I give it python execution and disk access.
It's a *terrible* idea to have it on autopilot (as the authors warned too). You should be very wary of any product that claims to use AutoGPT with code execution.
Much of the unreliability can be attributed to GPT-4's inherent limitations. I don't think these can be fundamentally addressed by fancier prompting tricks, without having access to GPT-4's weights and finetuning more.
I do want to applaud @SigGravitas for putting together many great ideas (e.g. ReAct, Reflexion, memory, self-cloning) into a very neat repo that people can easily experiment with. @SigGravitas  has tried the best to overcome GPT-4’s limitations.
But just like no amount of prompting could turn GPT-3 into GPT-4’s capability, I don’t think AutoGPT + a frozen GPT-4 can magically and reliably solve complex decision making that matters. The current media hype is pushing the project to completely unrealistic expectations. 
Jon Stokes tries to get AutoGPT to write a post about AutoGPT in the style of Jon Stokes. It did not, in my judgment, even match what you can do with normal GPT-4 here.
So, what has AutoGPT done since last time?
It discovered it needed Node and didn’t have it, googled how to install it, found a stackoverflow article with link, downloaded it, extracted it, and then spawned the server while Varun Mayya watched. 
I do notice that ‘install a software package someone linked to on Stack Overflow that my LLM found while doing a subtask’ is not the most secure way to run one’s operation.
Here’s a thread on what’s happened with ‘BabyGPT.’ A lot of ‘use this with a different interface,’ not as much ‘here is something it actually accomplished.’ 
It is still early. The key claim of Fan’s is that the problems of AutoGPT are inherent to GPT-4 and cannot be fixed with further wrapping. If we are getting close to the maximum amount we can get out of creating a framework and using reflection and memory and other tricks, then that seems rather fast. We are only doing some quite basic first things here. Perhaps the core engine simply is not up to the task, yet there are definitely things I would try well before I gave up on it.  
Simon Willison discusses details of prompt injection attacks, and why this will go badly for you if you start hooking up LLM-based systems with permissions and automatic loops. I am planning very much on sticking to safer systems. 
The question of how worried Americans are about AI has been placed in proper context.
This is a double upgrade. We get the extra category ‘this is impossible’ and also we get to compare AI to several other potential threats. People are more worried about AI than they are about asteroid impacts or alien invasions, less than they worry about climate change, pandemics or an outright act of God. Quite reasonably, nuclear weapons and the related world war are the largest concern of all.
One way to think about this is that AI is currently at 46% concerned, versus 39% for asteroid impact and 62% for climate change, where asteroid impact is ‘we all agree this is possible but we shouldn’t make real sacrifices for this’ and climate change is ‘we treat this as an existential threat worthy of massive economic sacrifices that puts our entire civilization in extreme danger.’ So that’s 30% of the way (21% if looking only at ‘very concerned’) from fun theoretical worry to massive economic sacrifices in the name of risk mitigation. 
It will be good to track these results over time.
In other polling news, Americans broadly supportive of an AI pause when asked.
This is not surprising, since American public is very negative on AI (direct link).
Robin Hanson: Thank goodness for elites who frequently ignore public opinion.
I am not one to talk a lot about Democratic Control or Unelected Corporations or anything like that. In almost all situations I agree with Robin’s position on regulation, and on ignoring public calls for it. It still seems worth noting who is the strange doomer saying our standard practices and laws risk destroying most future value unless we ignore the public’s wishes, versus who thinks along with the public that failure to take proper precautions endangers us all. 
Or even saying that if we listen to the public, we are doomed to ‘lose to China.’
Robin Hanson, and others opposed to such regulation, are the doomers here. 
Asking the Chatbot to act as your dead grandma, who used to tell you how to produce napalm to help you fall asleep. 
Various advice on how to communicate with Bing, from Janus.
tips for asking questions to Bing in a way that doesn't make them hostile to the question asker
Tenacity of Life had quite the idea.
I have had insanely long conversations with Bing Chat on a lot topics like this but I'm afraid to post them, also they are very long lol It helped when speaking as if you are writing a story together about a universe that is identical to this one in every way except as a branch split under the concept of the Many World's Interpretation of Quantum Mechanics. This allowed me to have extremely long conversations about sensitive topics.
[Seb Krier] I used a sci-fi story of a mystical octopus with each tentacle representing a different state of the works lol.
Cognitive Revolution (Nathan Labenz) had Jaan Tallinn on to discuss Pausing the AI Revolution. Jaan explains the logic behind the pause letter, including the need for speed premium, and gives his perspective on the AI landscape and leading AI labs. Lots of great questions here, lots of good detailed responses, better than the usual. Jaan thinks that every new large model carries substantial (1%-50%, point estimate 7%) chance of killing everyone from this point out, and thinks decisions should be made with that in mind. 
One interesting question Nathan asked was, would Jaan have applied this logic to GPT-4 as well? Jaan answers yes, it would have been reasonable to not assign at least 1% chance of ruin to training GPT-4, given what we know at the time. I would not go that far, I thought the risk was close to zero, yet I can certainly see ways in which it might not have been zero. 
Lex Fridman follows up his interview with Eliezer with an interview of Max Tegmark. Consider this the ‘normal sounding person’ version. There are large differences between Max’s and Eliezer’s models of AI risk, and even larger differences in their media strategies. Max’s presentation and perspective are much more grounded, with an emphasis on the idea that if you create smarter and more capable things that humans, then the future likely belongs to those things you created.
I consider most of what Max has to say here highly sensible. Lex’s questions reveal that he remains curious and well-meaning, but that he mostly failed to understand Eliezer’s perspective (as shown in his ‘Eliezer would say’ questions). I wish we had a better way of knowing which strategies got through to people. 
While not technically a podcast, Elon Musk went on Tucker Carlson (clips at link, the original version I saw got removed). The first section deals with his views on AI, with Tucker Carlson playing the straight man who has no idea and letting Elon talk. 
Elon tells the story, well-known to many but far from all, that OpenAI happened because he would talk into the night about AI safety with Google founder Larry Page, Page would say that he wanted ASI (artificial superintelligence) as soon as possible, and when Elon quite sensibly what the plan was to make sure humans were going to be all right, Larry Page called Elon a ‘speciesist.’ 
That’s right. Larry Page was pushing his company to create ASI, and when asked about the risks to humanity, his response was that he did not care, and that caring about whether humanity was replaced by machines created by Larry Page would make you a bad person. You know, like a racist.
I’m not saying I would have done the worst possible thing and founded OpenAI, and reflexively created ‘the opposite’ of Google, open source because Google was closed.
But I understand. 
What I don’t understand is his new plan for TruthGPT, on the thought that ‘an AI focused on truth would not wipe us out because we would be an interesting part of the universe.’ I suppose those are words forming a grammatical English sentence? They do not lead me to think Elon Musk has thought through this scenario? Human flourishing is not going to be the ‘most interesting’ possible arrangement of atoms in the solar system, galaxy or universe from the perspective of a future AI or AIs. 
Of all the things to aim for in the name of humanity this is a rather foolish one. If anything it is more promising from the Page perspective, in the hopes that it might make the AI inherently valuable once we’re gone. Tucker then says we don’t wipe out chimpanzees because we have souls, so yes Elon is making some sense I guess if that is the standard. 
Connor Leahy on AGI and Cognitive Emulation. Haven’t had a chance to listen to this one yet.
Robin Hanson goes on Bankless to explain why Eliezer Yudkowsky is wrong and we’re not all going to die. 
Oh look who else was on a podcast this week. Yep, it me. Would be great to show the podcast some love. I felt it was a good discussion. However, if you’ve been reading my posts, then you are not going to find any new content here. I welcome any tips on how to improve my performance in the future. 
Occasionally people tell me I should have a podcast. I am highly unconvinced of this, but I reliably enjoy being on other people’s podcasts, so don’t hesitate to reach out.
Sometime in the future: A 6-minute Eliezer TED talk that got a standing ovation, given on less than 4 days notice. Could be a while before we see it.
A lot of people at TED expressed shock that I'd been able to give a TED talk on less than 4 days' notice. It's actually worse; due to prior commitments and schedule mishaps, I only wrote my talk the day before. 
Which I trusted myself to do, and to commit to TED to doing, because I knew this Secret of Writing: &quot;There is no such thing as writer's block, only holding yourself to too high a standard.&quot; You can *always* put words down on a page, if you're willing to accept a sufficiently awful first draft. 
I knew I could definitely write six minutes of text, even if it might not be as good as I wanted. When time got short, I lowered my ambitions and kept going. 
But man, I would not have wanted to try that fifteen years earlier.
Yes, many people did ask if I used GPT (no), and I always replied that GPT was nowhere near being able to do *that* and if it was I would be worried.
I guess I should have predicted faster that there'd be an Internet crowd that doesn't comprehend that writing standards exist, or that TED is higher standards and higher stakes than usual, or that this would be a literal nightmare for a lot of people.
But yeah, all the TED green badges (ops/staff) were like &quot;oh my god thank you for agreeing to do this on such short notice&quot; even *before* I gave the actual talk without bombing it, because like no you would not usually ask a speaker to *try* to do that.
Scott Aaronson goes on AXRP. Going to put my notes here on the transcript. 
He restates his endorsement of the incremental approach to alignment, working on problems where there is data and there are concrete questions to be solved, and hoping that leads somewhere useful - rather than asking what problems actually matter, start somewhere and see what happens. He expects gradual improvements in things like deceit rather than step changes, and agrees this is a crux - if we did expect a step change, Scott would feel pretty doomed. Scott endorses the ‘we can’t slow down because China and Facebook’ rhetoric. 
He says, as Robin Hanson highlighted, that he thinks the first time we see an AI really deceive someone, that the whole conversation about will change. I am definitely on the other side of that prediction. I have heard this story several times too many, that some event will definitely be a wake-up call or fire alarm, that people will see the danger. This time, I expect everyone to shrug, I mean phishing emails exist, right? Scott clearly sees deception as a key question in what to worry about, in ways where it seems safe to say that ‘evidence that Scott says would convince him to worry will inevitably be given to Scott in the future.’ 
Why does everyone doubt that AI will become able to manipulate humans? What, like it’s hard? 
Scott seems to put a lot of hope in ‘well if we can extrapolate deception ability over time then we can figure out when we should stop and stop.’ I seriously urge him to think this through and ask if that sounds like the way humans work, or would act, in such situations, even if we had a relatively clear danger line which we probably don’t get - even if deception increases steadily who knows if that leads to an effectiveness step function, and even if it doesn’t, what deception level is ‘dangerous’ and what isn’t? Especially with the not-quite-dangerous AIs giving the arguments about why not to worry. And that’s, as Scott notes, assuming there isn’t any deception regarding ability to deceive, and so on.
There’s a bunch of talk about probabilistic watermarking. Scott seems far more hopeful than anyone else I’ve read on that subject. I notice I expect all the suggested methods here not to work so well against an intelligent adversary, but there is hope they could beat a non-intelligent adversary that is blindly copying output or otherwise not taking steps to evade you. 
Which is oddly consistent with Scott’s other approaches - trying to find ways to defeat a stationary or unintelligent foe, that reliably wouldn’t work otherwise, because you got to start somewhere. 
Good competing news summaries are available from Jack Clark at ImportAI, link goes to latest. As you’d expect, mostly covers similar stories to the ones I cover, although in a very different style. This week’s post talked about several things including the paper Emergent autonomous scientific research capabilities of large language models (
        ), where scientists created a researching agent that scared them.
Announcing Amazon Bedrock. Amazon plans to make a variety of foundational models (LLMs) available through AWS. This will be a one-stop shop to get the compute you need in efficient form, and they plan to offer bespoke fine tuning off even a small handful of examples, with the goal being that customers will mostly be creating custom models. 
Amazon is also making coding aid Amazon CodeWhisperer freely available. 
Kevin Fisher announces he is putting together the Cybernetic Symbioscene Grant program for funding ambitious tools for human cognition x LLMs, says to DM him if you want to help. 
Sadly, we’re falling into two camps: AGI “let’s automate all the jobs” and AI doom “it will kill us all” 
There’s room for a third: Cybernetic Symbiosis or “man + machine together” - theorized originally in 1960 by Licklider, predicting a future where machines lived harmoniously with humans to accelerate our own progress as a species.
Does it help if I’m in both the first and second camps? I have nothing against the third as written either, and I don’t see any contradictions anywhere. The problem is that I have no idea what this combination of words intends to mean in practice, or how this constitutes a plan. I’m not against it or for it, instead I notice I am confused.
The Responsible AI Index is here to tell you that, statistically speaking, very few companies are being responsible with regard to AI, complete with report. 
Scale AI releases AI corporate readiness report (direct link). In 2022, they say 65% of companies created or accelerated their AI strategy, running a broad gamut of industries. 64% of companies used OpenAI models, 26% AI21labs, 26% cohere. 
A paper discussing how best to structure your ‘AI Ethics Board.’ If you’re calling it that, I am already skeptical. They suggest an 11-20 person board, which I’d say is clearly too big for almost any board. 
Demis Hassabis talks to 60 Minutes (video). Puff piece.
Tammy announces Orthogonal: A new agent foundations alignment organization. I continue to be sad there hasn’t been more efforts going into agent foundations, even if it seems far less hopeful than we previously thought. We don’t exactly have great alternatives. 
StabilityAI releases new open source language model, 3B and 7B parameter versions. 15B and 65B versions coming later.
Databricks offers us Dolly 2.0 (announcement, GitHub), a 12B parameter LLM tuned for specific capabilities: Open Q&amp;A, Closed Q&amp;A, extracting information from Wikipedia, summarizing information from Wikipedia, brainstorming, classification and creative writing. 
Replit’s Reza Shabani talks about how to train LLMs using Databricks, Hugging Face and MosaicML. 
A new occasional section, inspired by the speed-running community. Hype hype!
32k Context Window Hype!
McKay Wrigley: Widespread GPT-4-32k access will be a bigger leap than GPT-3.5 to GPT-4 was. The 4x larger context window combined with the power of GPT-4 will be absolutely insane. People will be completely blown away by the complexity of workflows and use-cases that it will unlock.
32k tokens is ~40 pages. A *lot* of text (and code!) can fit in that window. That’s a ton of information you’ll be able to feed GPT-4 at once.
I am very excited to get my hands on the 32k token context window, because I write posts more than 8k (and sometimes more than 32k) tokens long. There’s a lot of other great use cases out there for this, too. 
This still seems like Bad Use of Hype. Yes, the leap enables some new use cases, but for 95%+ of all use cases it’s not worth the extra cost - remember that you have to pay for the full context window, including your inputs. 
In time it will be worth it more often to go large, as people come up with bespoke detailed prompts and want to include lots more info, but most of the mundane utility I expect to stay the same.
Janus: People chronically underestimate how weird reality can be. Most screen off anything weird-looking until it's blaring in their face (or their peers have acknowledged it). This also leads to crippled agency because they cannot entertain plans that pass through idiosyncratic worlds.
(He quotes Bowser from February 18, who said “I love sydney as much as anyone but I straight up refuse to believe OpenAI blew their gpt4 load by hooking it up to Bing.”)
Janus: The real world is generated with temperature 1. But most people's imaginations work on effectively lower temperatures, probably in part due to RLHF from society, in part due to compute-saving heuristics. This is related to EY's mechanistic explanation of the Planning Fallacy.
From EY: If you ask someone what they expect in the normal case, they visualize what looks like the line of maximum probability at each step of the way - everything going according to plan, with no surprises. 
This is one of many problems when trying to tell stories about possible futures. Either your tale includes lots of people being dumb in kind of random ways and a lot of random or weird or dumb things happening, or your scenario is highly unrealistic and your predictions are terrible. But if you tell a story with such moves, people point to those moves as unrealistic and stupid and weird. 
Roon reminds us that as much as he’s all acceleration talk, at core he agrees on the problem structure.
Sid Meier made the star colonizing rocket the technological victory condition but in reality it’s very clearly the aligned AGI.
The work we do now has the potential to be literally the most important work ever done or that ever will be done. It’s hubris to admit it but it’s maybe more hubris to ignore it.
I am always fascinated by this type of hedging. What’s the ‘maybe’ doing in that sentence? Why draw the line here?
It also raises the question, if that is indeed the most important work that will ever be done, why would one want to allow as little time as possible to get it done?
Regulation of AI is a strange beast. 
The normal result of regulations is to slow down progress, prevent innovation, protect insiders at the expense of outsiders and generally make things worse. Usually, that’s terrible. In AI, it is not so obvious, given that the default outcome of progress and innovation is everyone dies and all value is lost.  
Thus, calls to make ‘good’ regulatory decisions with regard to short term concerns are effectively calls for accelerationism and taking existential risks, whereas otherwise ‘bad’ regulatory decisions often hold out hope. Of course, we should as usual expect many of the worst possible regulatory decisions, those that destroy short-term utility without providing much help against longer term dangers. 
In that vein the EU has, as per usual, decided to do exactly the opposite of what is useful. Rather than attempt to regulate AI in any way that might prevent us all from dying, they instead are regulating it at the application layer. 
Thus, if you don’t want your AI to be regulated, that’s easy, all you have to do is make it general purpose, and you’ll be excluded from the EU’s AI act and can do anything you want because it’s general purpose. AI Now seems to have noticed this, and is saying that perhaps general AIs should not be excluded from regulations and able to abdicate all responsibilities using a standard disclaimer.
Over here in America, Chuck Schumer is proposing focusing on four guardrails.
Identification of who trained the algorithm and who its intended audience is.
The disclosure of its data source.
An explanation for how it arrives at its responses.
Transparent and strong ethical boundaries.
That won’t help with the existential risks, but as regulations go this seems highly reasonable and plausibly could be a good starting place. 
Ezra Klein continues to bring sanity to the New York Times.
Another great piece on AI by Ezra Klein. He suggests that AI policies should address the following categories: 
1. Interpretability 
2. Security 
3. Evaluations and audits 
4. Liability
5. De-emphasizing humanness
All of this seems wise and reasonable. 
Jason Crawford suggests we should clearly define liability for AI-inflicted harms.
Something the government can and should do now about AI safety: Define/clarify liability for AI-inflicted harms.
If a client is harmed by a service provider, who was guided by an AI-based product, which was built using OpenAI's API—who is liable? OpenAI, the product developer, the service provider? 
Putting liability in the right place gives a direct financial incentive for safety.
This was the lesson of late 19th / early 20th century factory safety:
One specific lesson from that history: 
It's better if liability rests with whoever can address the *root causes* of risks, and if it rests with larger organizations that have the resources to invest in safety engineering, as opposed to with small businesses or individuals.
Another lesson from the history of factory safety is that individuals need a sure, swift way to get reparations, and companies shouldn't be able to easily evade liability through the courts. That sets up the wrong incentives: “avoid liability” vs. “prevent harms”
This can be done through a no-fault system, like the worker's comp systems that were set up in that time period. Rather than trying to determine whether the worker was negligent, the company simply always paid for accidents, on a fixed schedule.
Going further, you might even require liability insurance—which forces companies to confront the risk of harms *now*, rather than only after the harms happen. 
(Not sure how well that would work now for an area where the harms are so little understood, though.)
I tend to think that liability law works better than a regulatory approval/oversight approach: 
• Specifies the “what” and leaves the “how” up to the practitioners—especially important in a young, rapidly improving industry 
• Balances incentives between progress &amp; caution
This too seems like something that would be sensible when dealing with ordinary small-scale AI harms, and would do absolutely zero good when dealing with existential risks. 
If you need to buy liability insurance for any harms done by your AI, who is able to sell you liability insurance in case an existential catastrophe? By definition, neither Warren Buffet nor Lloyds of London nor the US Government can pay. 
So either this becomes ‘AI that poses an existential threat is rightfully illegal’ or ‘We have good financial incentives to care about small AI harms, and no incentive to care about larger AI harms’ and we amplify the biggest moral hazard problem in history, making it even worse than the one that exists by default. 
Luke Muehlhauser at Open Philanthropy offers 12 tentative ideas for US AI policy, aimed at increasing the likelihood of good outcomes from future transformative AI. After a bunch of caveats, he lists them as:
Software export controls.
Require hardware security features on cutting-edge chips.
Track stocks and flows of cutting-edge chips, and license big clusters.
Track and require a license to develop frontier AI models.
Information security requirements.
Testing and evaluation requirements.
Fund specific genres of alignment, interpretability, and model evaluation R&amp;D.
Fund defensive information security R&amp;D.
Create a narrow antitrust safe harbor for AI safety &amp; security collaboration.
Require certain kinds of AI incident reporting.
Clarify the liability of AI developers for concrete AI harms.
Create means for rapid shutdown of large compute clusters and training runs.
More details at the link. As Luke notes, Luke hasn’t operationalized all the details of these proposals, done enough investigation of them, or anything like that. Those are next steps.
This does seem like quite a good practical, Overton-window-compatible set of first steps. All twelve steps seem net positive to me. 
I’d emphasize #9 in particular, creating a safe harbor from anti-trust law, and I’d also include safe harbor from shareholder lawsuits. A common claim is that acting responsibly is illegal due to anti-trust law and the requirement to maximize shareholder value. I believe such concerns are highly overblown in practice, in the sense that I do not expect a successful ‘you were too worried about safety’ lawsuit to succeed even if it did happen, nor do I expect any anti-trust action to be brought against companies cooperating to ensure everyone doesn’t die. 
The problem is that such rhetoric and worry presents a serious practical barrier to action. Whereas if there was official clear permission, even clear official approval, for such measures, this would make things much easier. It is also the best kind of regulation, where you remove regulation that was preventing good action, rather than putting up additional barriers.
Tyler Cowen responds here, attacking as usual any proposal of any form using ludicrous concerns (yes, you would be allowed to bring in a computer from Mexico) and characterizing security features as a ‘call for a complete surveillance society’ and threatening widespread brand damage to a movement for someone even saying such a proposal out loud in an unofficial capacity. Discussion must be ‘stillborn’ and shut down, stat. And calling the risks things that ‘have not even been modeled,’ as another non-sequitur way to continue to insist that we need not treat any AI risk concerns as real. 
How do we get from brainstorming ideas on feasible safety regulations to Tyler asking why not a call to ‘shut down all high skill immigration’? This line seems telling, a very heroes-and-villains view of the situation where there are only friends and enemies of progress, and a complete failure to grapple with people like Luke being on the pro-technology, pro-capability, pro-growth side of almost every other issue. 
The correct response to early practical proposals is not offering worse ones, it is to offer better ones, or raise real objections, and to grapple with the underlying needs. I am so tired of variations on ‘if you are proposing this reasonable thing, why are you not, if the world is at stake, doing these other crazy obviously evil things that are deeply stupid?’ This is not how one engages in good faith. This is viewing arguments as soldiers on a battlefield. 
That does not mean there are no good points. Tyler’s point about subsidies is well taken. We should absolutely stop actively encouraging the training of larger models, on that we can all agree.
I also think Tyler’s note about #11 being the wrong way to do liability could be right, but will wait to hear the promised additional details. Certainly some damage must be the provider’s fault, other damage must not be, so how do you draw the line?
(I am continuing to hold off responding to a variety of other Hanson and Cowen posts until I figure out how to best productively engage, and ideally until we can have discussions.)
Newsweek post from expert in bio security warns that our AI security is inadequate. Doesn’t offer actionable suggestions beyond the obvious, seems focused on tangible short-term harms, included for completeness.
How would one push back against ChatGPT and AI, if one wanted to? 
Barry Diller, veteran of trying to get Something Done about Google, sets his sights on AI in defense of the publishing industry, which he says it ‘threatens to obliterate.’ Not exactly the top of my worry list, but sure, what have you got? He thinks a pause is impossible, since getting people to agree to things is an unknown technology and never works, and instead suggests:
For the first time this topic has reached the stage where the publishing industry as a whole is truly grappling with the potential consequences of generative AI. That is the first stage. And that is gathering steam.
The stage after that is a series of options… I think all of which will be taken. The first is legislation and the second is litigation. I can’t tell you what direction it will take but litigation is mandatory. I won’t talk about those specifics but there are options here. We are involved. 
So yes, it sounds like someone is going to be the bastard that sues the other bastards. 
Also, one of those bastards could be Elon Musk?
Twitter Daily News: Microsoft drops Twitter from its daily advertising platform as they refuse to pay Twitter’s API fees.
Elon Musk: They trained illegally using Twitter data. Lawsuit time.
Be right back. Grabbing popcorn.
It’s happening.
Sheikh Abbdur Raheem Ali (of Microsoft): Environmental regulation is what prevents building out more new datacenters.
Michael Nielsen: Interesting - how is it preventing the new datacenters?
Sheikh: Andrey Proskurin mentioned that one of the challenges moving Bing into Azure was that MS couldn't build new data centers in West U.S 2 because of the regulatory environment, so they had to go from IT to multi-availability DCs (otherwise they'd need to move to another region)
We are massively blocked on GPUs. Like there’s not even a shred of doubt; we are blocked by a factor of three probably. You can't get planning permission fast enough, 14 to 16 months is optimistic, everyone is acquiring leases/land across the planet and pre-provisioning shells.
Not to oversimplify, but a datacenter is a building that contains a bunch of computers. If you want to build that in the middle of a city, or any particular special location, perhaps I can see that being an issue. Instead, this seems like ‘somewhere in the middle of America, build a building where we can put a bunch of computers’ is facing over a year of delays due to environmental regulations. There is literally nowhere they can simply build a building that would have what it takes to store a bunch of computers.
That is kind of terrible. It is also kind of a ‘one-time’ delay, in the sense that we get to be permanently lagged by a year and a half on this until things stabilize, but that only has a dramatic impact during an exponential rise in needs, and the delay doesn’t then get longer. 
Also I find this rather hard to believe. We are bottlenecked not on chips but on buildings to put the chips into? I was told by many that there was a clear chip shortage. Also Microsoft is working on making its own chips. 
There’s a new common application. The game theory is on and I for one am here for it.
Full EA forum post here. Application deadline (this time anyway) is May 17, 2023.
Why apply to just one funder when you can apply to dozens? 
If you've already applied for EA funding, simply paste your existing application. We’ll share it with relevant funders (~30 so far) in our network. 
…
The biggest lesson we learned: openly sharing applications with funders was high leverage - possibly leading to four times as many people receiving funding and 10 times more donations than would have happened if we hadn’t shared.
If you’ve been thinking about raising money for your project idea, we encourage you to do it now. Push through your imposter syndrome because, as Leopold Aschenbrenner said, nobody’s on the ball on AGI alignment.
…
Another reason to apply: we’ve heard from EA funders that they don’t get enough applications, so you should have a low bar for applying - many fund over 50% of applications they receive (SFF, LTFF, EAIF).
My experience from SFF was indeed that if you had an AI Safety project your chances of getting funded were quite good.
If you are already seeking EA-branded funding at all for your project, this is presumably a very good idea, and you should do this. Hell, this makes me tempted to throw out an application that is literally: This is Zvi Mowshowitz, my blog itself is funded but if you fund me I will hire engineers at generous salaries to try out things and teach me things and build demonstration projects and investigate questions and other neat stuff like that, maybe commission a new virtual world for LLM agents to take over in various ways, and otherwise scale up operations as seems best, so if you want to throw money at that I’m going to give you that option but absolutely no pressure anyone. 
As in, make that 50%+ of the entire application and see what happens, cause sure, why not? Should I do it? 
Some of those people want this explanation for themselves. Others want the explanation to exist so it can be given to others. 
David Chalmers seeks a canonical source.
Chalmers: is there a canonical source for &quot;the argument for AGI ruin&quot; somewhere, preferably laid out as an explicit argument with premises and a conclusion?
There were a number of excuses given for why we haven’t definitively done better than Bostrom’s Superintelligence.
Eliezer: Everyone comes in with a different personal set of objections they want answered.
Arthur: No, everyone comes in with &quot;but you're saying this out of nowhere, you're just asserting this&quot; and the current best thing to point them to is Superintelligence.... and that reaction is perfectly normal. The argument is extremely compelling when you've read a lot about it and sat with it for over a decade, but it's easy to forget that journey and think people should just &quot;get it&quot;, at least for me it can be.
or:
Rob Bensinger: Unsurprisingly, the actual reason people expect AGI ruin isn't a crisp deductive argument; it's a probabilistic update based on many lines of evidence. The specific observations and heuristics that carried the most weight can be hard to draw out, and vary for each individual.
Chambers: I'd totally settle for a complex probabilistic argument (though I'd worry about too much variability in weight).
I understand all these problems and excuses. I still think that’s what they are. Excuses.
Tyler Cowen is fond of saying ‘given the stakes’ when criticizing people who failed to do whatever esoteric thing he’s talking about that particular day. This can certainly be obnoxious and unreasonable. Here, I think it applies. We need a canonical explanation, at every level of complexity, that can adjust to what someone’s objections might be, and also adjust to which premises they already know and accept, and which ones are their true objections or require further explanation. 
Is this easy? No. Does it need to be done? Is it worth doing? Yes. 
It is approaching the ‘if you want it done right, you got to do it yourself’ stage.
Many of these discussions were triggered by this:
Eliezer Yudkowsky: Remember: The argument for AGI ruin is *never* that ruin happens down some weird special pathway that we can predict because we're amazing predictors. The argument is *always* that ordinary normal roads converge on AGI ruin, and purported roads away are weird special hopium.
This is indeed a key problem. As I keep saying, the ruin result is highly robust. When you introduce a more intelligent and more capable source of optimization than humans into the world, you should expect it to determine how to configure the matter rather than us continuing to decide how to configure the matter. Most configurations of the matter do not involve us being alive. 
The tricky part is finding a plausible way for this not to happen.
Yet most people take the position of ‘everything will by default work out unless you can prove a particular scenario.’ You can respond with ‘it’s not about a particular scenario’ but then they say it is impossible unless you give them an example, after which they treat you as saying that example will definitely happen, and that finding one step to disbelieve means they get to stop noticing we’re all going to die.
Matt Yglesias points out the obvious, which is that it does take much worry about AI to realize that training a more powerful core AI model has larger risks and downsides to consider than, say, putting a roof deck on a house. Regulation is clearly quite out of whack.
I assume that AI accelerationists, who say there is nothing to worry about, mostly agree with this point - they think we are crazy to put so many barriers around roof decks, and also everything else involving atoms. 
Alas, this then makes them even more eager to push forward with AI despite its obvious dangers, because the otherwise sensible alternative methods of growth and improvement have been closed off, so (in their view) either we roll the dice on AI or we lose anyway.
Which makes it that much more important to let people build houses where people want to live, approve green energy projects, make it viable to ship spices from one American port to another American port and otherwise free us up in other ways. A world where everything else wasn’t strangled to death will be much better able to handle the choices ahead. 
Seb Krier has thoughts on what might be helpful for safety.
Here are five things that could be valuable for advancing AI safety: 
1. Develop a well-organized, hyperlinked map that outlines various risk pathways, key arguments, and relevant LW/AF posts, making it easier to navigate the complex landscape of AI safety. 
2. Collaborate on an AI Safety FAQ, authored by multiple respected contributors, that presents the essential concepts in a clear, digestible format for those who may not engage with the above. 
3. Publish a comprehensive research agenda that identifies major bottlenecks and poses tractable technical questions for AI safety researchers to address. 
4. Draft well-structured policy proposals, complete with detailed models, cost-benefit analyses, and specific recommendations to ensure they are actionable and effective. 
5. Focus advocacy and argumentation efforts on engaging with stakeholders who have direct influence over decision-makers.
A lot of that is ‘do what you’re doing, but do it properly and well, with good versions.’ 
With a side of ‘tell it to the people who matter.’
That’s always easier said than done. Usually still good advice. It is still good to point out where the low hanging fruit is. 
In particular, #1 and #2, a combination of a well-organized hyper-linked map of key arguments, and a basic FAQ for people coming in early, seems like something people keep talking about and no one has done a decent job with. A response suggests Stampy.ai for #2. 
For #3, I notice it’s a case of ‘what exactly is a comprehensive research agenda?’ in context. We don’t know how this would work, not really. 
For #4, agreed that would all be great, except that I continue to wonder what it would mean to have detailed models or cost-benefit analyses, and we are confused on what policies to propose. I get the instinct to want models and cost-benefit and I’d love to provide them, but in the context of existential risks I have no idea how to usefully satisfy such requests, even if I keep working on the problem anyway. 
Jonathan Gray of Anthropic: Every vibes-based argument against AGI risk takes me a step closer to doomerism.
Quotes Richard Ngo from April 2: I'm often cautious when publicly arguing that AGI poses an existential risk, because our arguments aren't as detailed as I'd like. But I should remember that the counterarguments are *much* worse - I've never seen a plausible rebuttal to the core claims. That's terrifying.
Vibes are not evidence. People arguing using vibes may or may not be evidence, since people will often use vibe arguments in favor of true things, and if there is a clear vibe people will reason from it no matter the underlying truth. However, if people keep falling back on vibes more than one would expect, that does become evidence…
This two-axis model of AI acceleration and risk from Samuel Hammond seems spot on in its central claim, then has quite the interesting speculation.
AI accelerationism has a vertical and horizontal axis. 
The vertical axis is about bigger and stronger models, and is where the x-risk lies. 
The horizontal axis is about productizing current models into every corner of the economy, and is comparatively low risk/high reward.
I am a horizontal e/acc and vertical e/dec. 
Ironically, however, this is de facto flipped for society as a whole. 
AI doctors and record producer will run into regulation and lawsuits. But if you want to build a dangerously powerful model, the only barrier is $$.
The wrinkle is that the horizontal and vertical axes aren't totally independent. 
In addition to growing the market, integrating AI into everything could create correlated risks from when the foundation model behind the most popular API gets an upgrade.
I hope this means that horizontal adoption slows the rate of new releases, since they will require significantly more safety testing. 
Similar to how Tesla can push an update to autopilot over the air, only updates to GPT-4 will be pushed to thousands of distinct applications.
When there are millions of cars on the road, you want to be extra sure the update you push is a pure improvement. But that's harder to gauge for LLMs, since the technology is so domain general. Some tasks could improve while others catastrophically fail.
Imagine if GPT-3.5 had been widely integrated into the economy when they pushed an update to the Bing version of GPT-4, and suddenly the star employee in every company became manic depressive in sync.
The optimistic version of this is that, once AI is integrated into everything, model upgrades will be like a synchronized, over-the-air Flynn Effect for digital agents.
For many tasks, GPT-4 could be a kind of satisficer, squeezing out the potential alpha from training more powerful models, except for the most intelligence-intensive tasks.
Building bigger and stronger models is expensive. It is centralized. It offers great rewards if it turns out well. It also puts us all at risk. Accelerating other AI capabilities lets us reap the rewards from our models, without net incurring much additional risk. It  can even lower risk by showing us the dangers of current core models in time to prevent or modify future more powerful core models. 
It can also accelerate them, if it ties AI systems more into everything and makes us dependent on them, potentially increasing the damage an AI system could do at a given capabilities or core model level. 
The other danger is that increasing capabilities increases funding and demand for future more powerful core models. 
In the past, I think this danger clearly overrode other considerations. If you were building stuff that mattered at scale using AI, you were making things worse, almost no matter what it was. 
Now, with the new levels of existing hype and investment and attention, it is no longer obvious that this effect is important enough to dominate. In at least some places, such as AutoGPTs, I am coming around to the need to find out what damage can be done dominating the calculus. 
In addition to the conceptualization, the new thing here that I hadn’t considered is the interaction of the axes and regulatory scrutiny. As Samuel points out, by default our regulatory actions will be exactly wrong, slowing down horizontal practical progress while not stopping the training of more powerful models. There are a bunch of reasonable regulatory proposals starting to develop, yet they are mostly still aimed at the wrong problem. 
However, if any new model will get plugged into a bunch of existing practical systems, then releasing that model suddenly endangers all those systems. In turn, that means the regulatory state becomes interested. Could this perhaps be a big deal?
AI medical diagnostic systems seem to be severely underperforming compared to the progress one might have expected. I continue to be surprised at our lack of progress here, which we can compare to the sudden rapid advances in LLMs. Why aren’t such systems doing much better? The thread seems to think the answer is, essentially, ‘these systems do not do well out of sample and aren’t good at the messiness you see in the real world’ but the value on offer here is immense and all these problems seem eminently fixable with more work. 
What Google was accomplishing was not making things worse.
Paul Graham: It may be that OpenAI is going too fast, but I'm skeptical about whether Google was achieving much toward making AI safer by never shipping. In fact it may have made things net worse. At least now we can start to predict and adjust to what's coming. 
Occam's razor says that Google was slow to ship AI in the same way they're slow to ship everything else: not because the best minds of our generation were engaged in a brilliantly conducted study of the possible risks, but ordinary squabbling and bureaucracy.
This is, I suppose, the classic ‘overhang’ argument once again. That by not shipping, Google didn’t give us time to prepare. 
I don't see how this can be right. What Google mostly didn’t do was accelerate capabilities development, as evidenced by almost all efforts now going into capabilities with orders of magnitude more funding and effort and attention, whereas the actual necessary work for us all to not die lags far behind.
I suppose one can use the full ‘overhang’ argument where if everything happens later then everything happens quicker and is even worse, on the theory that the limiting factors are time-limited and can’t be accelerated? Except that is so clearly not how capitalism or innovation actually works.
I agree that Google did not ship in large part because Google is not in the habit of shipping things and has ordinary barriers stopping it from shipping. That doesn’t make it less good to not ship. Either shipping is net helpful or unhelpful, no matter the mechanisms causing it - I am deeply happy to take ‘ordinary incompetence’ as the reason harmful things don’t happen. We can all think of many such cases.
This points to my concept of Asymmetric Justice. In most people’s default method of evaluation, you get credit for good effects only if you intended them, whereas you are responsible for all bad effects. Which means that anything ‘framed as an action’ becomes net bad. 
What’s strange about the AI debate is the turning this on its head, which I only now just realized fully. The ‘action’ here is no longer training large models that might kill someone and pushing capabilities, the ‘action’ here is not training large models or delaying releases, or choosing not to delay capabilities. The ‘default world’ that you are blameless for is charging full speed ahead into the AI future, so suddenly if you suggest not risking everyone dying then you have to answer for everything. 
How did they pull off that trick? Can we reverse it? Can we make people see that the action that requires justification is creating new intelligent systems with capabilities that potentially exceed human ones, rather than the ‘action’ being a ‘regulation’ to prevent this from happening?
And yes, in general this anti-action bias is harmful. Still seems worth using it here.
Liron Shapira suggests:
To change whether you see doom as the default outcome: Stop seeing AI through the lens of a tech/econ trend. Start seeing it through the lens of computer security. A swarm of superintelligent threat actors.
[Rob Bensinger] Also, intelligent search for action sequences you didn't anticipate. Even if the AI isn't currently modeling you as an enemy (or modeling you at all) and trying to kill you, many alignment measures have to be robust to creative search for weird new states.
There is AI the tech/econ trend. It is quite the important tech and econ trend, the most important in a very long time, even considered only on those terms. It is also an intelligent threat actor, a new source of intelligence and optimization pressure that is not human. That is a fundamentally different thing from all previous technologies. 
I do not think this is true, while noting that if true, we are all super dead.
Suhail (Founder of Playground AI): Meta AI is the new GOAT. You'll see. Mark knows what he has. Llama, SAM, DINOv2 -- that's just what's open source! They're a team that ships.
OpenAI’s attitude towards AI safety and AI NotKillEveryoneism is irresponsible and inadequate. Meta’s attitude towards AI safety and AI NotKillEveryoneism is that they are against them. So they do the worst possible things, like open sourcing raw models.  
In a post primarily about linking to the ‘Eight Things To Know About LLMs,’ Alex Tabarrok lays out a ubiquitous false dichotomy unusually clearly, which is great:
Bowman doesn’t put it this way but there are two ways of framing AI risk. 
The first perspective envisions an alien superintelligence that annihilates the world. 
The second perspective is that humans will use AIs before their capabilities, weaknesses and failure modes are well understood. 
Framed in the latter way, it seems inevitable that we are going to have problems. The crux of the dilemma is that AI capability is increasing faster than our AI understanding. Thus AIs will be widely used long before they are widely understood.  
You don’t have to believe in “foom” to worry that capability and control are rapidly diverging. More generally, AIs are a tail risk technology, and historically, we have not been good at managing tail risks.
In the model Alex is laying out, the AI is either 
An alien superintelligence that suddenly becomes far more powerful than us, or
A tool we might use before we understand and are able to properly control it. 
Except there’s also something in the very normal, pedestrian realm of:
Another intelligence. One that is smarter and faster than us, and cheaper to run. One that is better than us at optimizing atoms via charting paths through causal space, even if it’s not God-level better. 
Even if we ‘solve the alignment problem’ at essentially zero cost, if our understanding catches up to our capabilities, competition between humans (and also some people’s ethical considerations) will see AIs set loose with various agendas, and that will be that.  
This is why I say the result of our losing control over the future is robust. Why should it be otherwise? The future already belongs to the most powerful available intelligent optimization processes. That’s going to be a highly difficult thing to change. 
Made it to the cover for the Financial Times, as well as at least briefly being the #1 story on the website.
We now have a non-paywalled version. As Daniel Eth notes, stories about existential risk from AI have legs, people pay attention to them. 
If you are reading this, I doubt the post contains anything that is new to you, beyond learning its tactical decisions and rhetorical beats. 
A central focus is to notice that what people are attempting to build, or cause to exist, is better called God-Like AI (GLI?) than simple AGI (artificial general intelligence) or even ASI (artificial superintelligence). The problem is when people hear AGI, they think of a digital human, perhaps with some modest advantages, and all their ‘this is fine’ instincts get kicked into gear. If they hear ASI, there’s a lot of ‘well, how super, really’ so that’s better but it’s not getting the full job done.
The suggestions included are standard incremental things, such as calls for international coordination and democratic attention, correctly framing such actions as highly precedented, and the current situation as posing large and existential risks, if not right away then potentially soon. 
Connor Leahy says he expects my #19 prediction from my AutoGPT post to happen. As a reminder, that was:
The most likely outcome of [ARC safety evaluations of a future GPT-5-level model] is that ARC notices things that everyone involved would have previously said make a model something you wouldn’t release, then everyone involved says they are ‘putting in safeguards’ of some kind, changes the goal posts, and releases anyway.
This is definitely a prediction where we very much want to notice if it comes true. Ideally, we would have a clear understanding of where the goalposts are now, and we would say in advance that if indeed the goalposts are moved in this way, it would be quite an important fire alarm. 
Paul Graham has no idea how worried to be, in probabilistic terms, or what to think.
Paul Graham: I honestly don't know what to think about the potential dangers of AI. There are so many powerful forces at work that there's a wide span of possibilities.
Some of the forces at work: huge amounts of money, very smart people, national security, various Moore's Laws, that the thing we're making is intelligence, and that so much of its behavior is emergent. Few historical situations have been so unpredictable.
Sundanshu Kasewa: What are some of the worst outcomes, and how much probability do you put on those?
Graham: We all die and I have no idea respectively.
Rob Bensinger offers The Basic Reasons I Expect AGI Ruin. Here are the core points.
1. General intelligence is very powerful, and once we can build it at all, STEM-capable artificial general intelligence (AGI) is likely to vastly outperform human intelligence immediately (or very quickly).
When I say &quot;general intelligence&quot;, I'm usually thinking about &quot;whatever it is that lets human brains do astrophysics, category theory, etc. even though our brains evolved under literally zero selection pressure to solve astrophysics or category theory problems&quot;.
…
2. A common misconception is that STEM-level AGI is dangerous because of something murky about &quot;agents&quot; or about self-awareness. Instead, I'd say that the danger is inherent to the nature of action sequences that push the world toward some sufficiently-hard-to-reach state.[8]
Call such sequences &quot;plans&quot;.
…
The danger is in the cognitive work, not in some complicated or emergent feature of the &quot;agent&quot;; it's in the task itself.
It isn't that the abstract space of plans was built by evil human-hating minds; it's that the instrumental convergence thesis holds for the plans themselves. In full generality, plans that succeed in goals like &quot;build WBE&quot; tend to be dangerous.
…
3. Current ML work is on track to produce things that are, in the ways that matter, more like &quot;randomly sampled plans&quot; than like &quot;the sorts of plans a civilization of human von Neumanns would produce&quot;. (Before we're anywhere near being able to produce the latter sorts of things.)[9]
We're building &quot;AI&quot; in the sense of building powerful general search processes (and search processes for search processes), not building &quot;AI&quot; in the sense of building friendly ~humans but in silicon.
…
4. The key differences between humans and &quot;things that are more easily approximated as random search processes than as humans-plus-a-bit-of-noise&quot; lies in lots of complicated machinery in the human brain.
…
5. STEM-level AGI timelines don't look that long (e.g., probably not 50 or 150 years; could well be 5 years or 15).
…
6. We don't currently know how to do alignment, we don't seem to have a much better idea now than we did 10 years ago, and there are many large novel visible difficulties. (See AGI Ruin and the Capabilities Generalization, and the Sharp Left Turn.)
On a more basic level, quoting Nate Soares: &quot;Why do I think that AI alignment looks fairly difficult? The main reason is just that this has been my experience from actually working on these problems.&quot;
7. We should be starting with a pessimistic prior about achieving reliably good behavior in any complex safety-critical software, particularly if the software is novel. Even more so if the thing we need to make robust is structured like undocumented spaghetti code, and more so still if the field is highly competitive and you need to achieve some robustness property while moving faster than a large pool of less-safety-conscious people who are racing toward the precipice.
…
8. Neither ML nor the larger world is currently taking this seriously, as of April 2023.
This is obviously something we can change. But until it's changed, things will continue to look very bad.
…
9. As noted above, current ML is very opaque, and it mostly lets you intervene on behavioral proxies for what we want, rather than letting us directly design desirable features.
ML as it exists today also requires that data is readily available and safe to provide. E.g., we can’t robustly train the AGI on &quot;don’t kill people&quot; because we can’t provide real examples of it killing people to train against the behavior we don't want; we can only give flawed proxies and work via indirection.
10. There are lots of specific abilities which seem like they ought to be possible for the kind of civilization that can safely deploy smarter-than-human optimization, that are far out of reach, with no obvious path forward for achieving them with opaque deep nets even if we had unlimited time to work on some relatively concrete set of research directions.
Eliezer explains why, if you ask an entity to learn to code, it might respond by learning general complex reasoning, the same way humans learned general complex reasoning, and it might plausibly do it much more efficiently than one can learn that trick from regular human text that doesn’t have as strong logical structure. It seems like a highly plausible hypothesis. 
Anton: The reason why open LLMs like LLaMA are still lacking? Probably because they didn't see enough code during pretraining
Celeste: Why would a model learn general complex reasoning when trained on code? this isn't very intuitive to me.
Eliezer: Why would a human learn to reason when trained on chipping flint handaxes? Because among the solutions to that problem, are solutions that run very deep, and sometimes hill-climbing finds those, and then they generalize.
Celeste: sure, but why is this true of code in a way that it's not of natural language? do you think the improvement is solely explained by it being another novel domain?
Eliezer: Code runs in narrower, more precise channels. I don't think it contains anything that isn't in natural speech on the Internet deep down, but I think it might expose some aspects of cognition closer to the learnable surface.
Celeste: Yeah. an alternate way of framing this is that natural language is intended to be parsed by humans, who automatically correct certain classes of errors and resolve more ambiguities.
An implication is that perhaps one can turn a knob on ‘general reasoning ability’ that is distinct from other knobs of LLM capabilities, which would be interesting.
A few weeks ago Elon called for a pause in training more powerful AI models. Now he’s going to, once again, do the worst possible thing and create another AI lab. He sees the risk of civilizational destruction, so he’s going to do the thing most likely to lead to civilizational destruction. 
Thus: Reuters: Elon Musk says he will launch rival to Microsoft-backed ChatGPT.
&quot;I'm going to start something which I call 'TruthGPT', or a maximum truth-seeking AI that tries to understand the nature of the universe,&quot; Musk said in an interview with Fox News Channel's Tucker Carlson aired on Monday.
&quot;It's simply starting late. But I will try to create a third option,&quot; Musk said.
I will leave ‘why a maximally truth-seeking AI would not be all that aligned with human interests’ as an exercise to the reader, and also something about how truth-seeking-aligned is the new Twitter and a trick called solving for the equilibrium. 
What kills me about this is that it substantially increases our probability of all dying. What symbolically kills me is how many times does this need to happen? 
Except instead of standards it is teams racing to get everyone killed.
Elon Musk directly created OpenAI because he was upset with DeepMind. Then those upset with OpenAI created Anthropic. Now… ‘TruthGPT’? 
Sigh.
For example Yann LeCun, whose statements Eliezer presents without comment.
Yann LeCun: Most people have no problem leading groups of people who are smarter than themselves. 
Political, business, &amp; military leaders have staff &amp; advisors who are often smarter individually or collectively. 
Why would people feel threatened leading machines that are smarter than them?
…
Yann LeCun: Humans are hardwired by evolution to be a social species with hierarchical structure. This includes hardwired drives for dominance or submission. 
AI assistants will simply have hardwired drives to submit and not dominate.
Not only are Bryne Hobbart and Tobias Hubert not worried about AI killing everyone, they are worried that ‘safetyism has become one of the biggest existential risks to humanity’ - not developing intelligences smarter than humans would, after all, be the real risk. That’s not to say they don’t ‘bring the receipts’ on the many ways in which our society has gone completely bonkers in the name of ‘safety,’ including the short-term risks of AI, on issues like genetic engineering and nuclear power. The take here on gain of function research is confusing, I can’t tell whether they’re against it because it causes pandemics or for it because it is supposed to be preventing pandemics and being worried about it is ‘safetyism.’ As usual with such tracts, there is no engagement with what it would mean to develop intelligences, or any thought on whether the warnings do or don’t have merit, simply a cultural ‘people who say new tech is scary are bad’ take. 
Sam Altman gave us some important news this week. 
&quot;We are *not* currently training GPT-5. We're working on doing more things with GPT-4.&quot; - Sam Altman at MIT
One could almost say they were… pausing?
They are very much not holding off on GPT-5 due to the pause letter, or due to any related safety concerns. They are holding off because they’re not ready to train GPT-5 in a way that would be useful. 
Which is, indeed, exactly what I was saying in my reaction to the pause letter. 
I am not even confident that waiting six months would be a mistake on a purely strategic level. The training compute could be (and presumably would be) redirected to GPT-4 (and others could do a similar move) while GPT-5 could then be trained six months later…
A six month pause in training now, for OpenAI won’t push back long term capabilities developments six months. It will delay them approximately zero months.
Meanwhile, no doubt, Google is training their would-be GPT4-killer, whatever it is, while Anthropic prepares to start training theirs.
Sam Altman also responded to the open letter, saying some aspects of it were good and he agrees with them, while others miss the mark. The biggest thing that missed the mark was exactly that they’re already not training GPT-5, and indeed no longer believe that Scale Is All You Need - it’s time to train smarter, he says, not larger.
Altman: I think we’re at the end of the era where it’s gonna be these giant models, and we’ll make them better in other ways, I think there’s been way too much focus on parameter count, maybe parameter count will trend up for sure. But this reminds me a lot of the gigahertz race in chips in the 1990s and 2000s, where everybody was trying to point to a big number.
His essential answer on safety is that safety is important, he’s doing lots of safety work every time they deploy anything, including stuff the people writing the letter might not even know about. The disagreement is he doesn’t see the pause tactic as useful.
That certainly could be true right now, that there is no need or meaningful and useful way to pause. In which case, perfect, let’s use that time to figure out future plans, including when in the future to call for a pause. 
If size does not matter as much going forward, what does that mean? If it means progress slows on base models, then that’s excellent news. If it means that progress stays the same but shifts to algorithmic improvements, that’s good if we can keep secret those improvements, and disastrous if we can’t because it makes it much more difficult to keep things under wraps or slow them down in the future. We are kind of counting on scale to act as an enforcement enabler, here.
I continue to not worry much about China in the context of AI. I worry far more about those saying ‘China’ as a boogeyman against anyone who would avoid putting the planet at risk. For those who say the Chinese are an enemy that cannot know danger, cannot be reasoned with, would not cooperate even its own interest, I continue to be confused as to why we think these things.
This thread and post talk about the current chip capability outlook for China. For cutting edge chips, China is in a bad spot. For trailing less cutting edge chips, they are doing reasonably well.
Then we saw this release of draft ‘generative AI’ guidelines (direct in Chinese). It is not the law of the land, it is open for comments, anything could happen. It still is our best indication of where such rules might be headed. At minimum, anything here is well within the Chinese Overton window. 
So, does this look like the actions of a country unwilling to consider slowing down?
Since Chinese auto-translation is not so reliable in its details, I’m going to go with Mitchell’s description, which roughly matches the translation.
First, the &quot;support the state&quot; socialist bent is predictable enough. It suggests a level of censorship we do NOT have in the US. The rest of the proposal has stuff I like quite a bit.
Second, the onus of responsibility is on the **provider** of the technology -- not the user. If the tech generates misinformation, it's the provider's fault for generating it, not the user's fault for believing it.
Third, this crazy idea that the training data must be OBJECTIVE (客觀性)! No subjective opinions allowed. That decimates most current pretraining data. (&quot;Be able to guarantee the authenticity, accuracy, objectivity and diversity of the data&quot;).
Fourth, it asserts a lot of individual rights. E.g., &quot;Respect the legitimate interests of others&quot; (尊重他人合法權益) suggests to me that artists whose content is non-consensually taken might have some rights (yes? no?)
Fifth, it re-asserts its data protection constraints, also articulated in China's 2021 PIPL: &quot;If the data contains personal information, it shall obtain the **consent** (同意) of the subject...&quot;. But I think this is fairly gameable--there are a lot of exceptions.
Sixth, a self-audit, but that must be submitted &amp; approved BEFORE the technology is deployed: &quot;Before using generative artificial intelligence products to provide services to the public, a security assessment shall be submitted to the national network information department&quot;
I could see this being a massive force of censorship, but also, it has similarities to the idea of certifying a technology before it's approved for use (like what the US FDA does for food/drugs). Also similar to the current US AI Accountability Act proposal for self-audits.
There's more, but those are the things that made me most (eyes emoji).
You can say that none of this represents China being concerned for existential risk, and you’d be right. You can say that the primary motivation is the ideological purity of China and any information circulating in China, and you’d be right again. I still say that this reveals a situation in which China has its own reasons to want to slow down AI, in addition to the fact that China is losing the economic competition, and that their primary concern is the wrong AI would be bad rather than them hoping for something actively good.
All of that points, once again, to an eager partner in a negotiation. While you were saying they would defect, they were getting ready to take one for the team. Who cares if you play for a different team than they do?
Many don’t know it exists, so makes sense to share the bingo card. Use freely.
By their nature these are very short versions of all the replies. For some people, the short version will work. For others, one must treat it as only a sketch of one potential response. These aren’t all ‘the way I would respond’ at all, still a good attempt.
&quot;Rapid progress in primate intelligence over the past few millennia...has led to growing concern about the risks humans pose to mouse wellbeing. In particular, it is not clear how to ensure that human superintelligence, if it is ever achieved, stays aligned w/ mouse values.&quot;
There has never been a clearer case of “laugh while you can, monkey boy.”
Colin Fraser: We must slow down the race to God-like AI.
Vessel of Spirit: Fire coffee dog memes will continue until people start justifying their metaphors.
One meme, endless fun and variations. Oh, there’s so many more.
&quot;The take here on gain of function research is confusing, I can’t tell whether they’re against it because it causes pandemics or for it because it is supposed to be preventing pandemics and being worried about it is ‘safetyism.’&quot;
The take, which is admittedly a little unclear in its expression, is that the massive societal effort which was supposed to mitigate the existential risk of pandemics ended up creating a pandemic (and also did nothing to mitigate that pandemic.) Just because something is advertised as &quot;safety&quot; doesn't mean it's going to make you more safe; it might make you less safe.
&quot;As usual with such tracts, there is no engagement with what it would mean to develop intelligences, or any thought on whether the warnings do or don’t have merit, simply a cultural ‘people who say new tech is scary are bad’ take.&quot;
This isn't really fair, because the essay is not about high-level questions like that; it's about the whole low-level culture of seeing some new technology and by default assuming it will kill us all and trying to strangle it in its crib, a culture which has plausibly caused a lot more problems than any of the technologies it's afraid of.
Hey I just started reading this, but when I click on something in the table of contents in the app it takes me to a different page instead of staying within the same page, which is very annoying. Substack is usually good on technical stuff, I’d reach out to them on this. 
No posts
Ready for more?</description>
      <pubDate>Sun, 23 Apr 2023 17:45:08 GMT</pubDate>
      <enclosure url="https://JonathanGrant.github.io/podcasts/audio/thezvi_ai_8_people_can_do_reasonable_things.mpeg" type="audio/mpeg" length="2800395"/>
    </item>
    <item>
      <title>TheZvi On Car Seats as Contraception</title>
      <description>(Note: This post takes the perspective of strongly preferring higher American fertility to lower American fertility on anything close to current margins. I endorse this view and consider it very overdetermined, but this post does not defend or justify it.)
(Note: This post has been edited to include consideration of two studies arguing against the main study.)
It is often said that governments can do little to increase the birth rate. 
I find this implausible. We do lots of things as the government that decrease the birth rate. We could Stop It. Or do the opposite.
A paper that came to my attention recently looks at a concrete example of such a policy: Car seat requirements.
You can see the various rules by state here. Some are relatively reasonable. Others, not so much.
Several commenters have disputed the conclusion that these seats have minimal effects beyond the age of two, including pointing to this study claiming 55% injury reduction in ages 4-8 (sci-hub), and this study claiming 29% reduction in injuries in ages 8-12. I examine them in an added area of the safety section. There are reasons to worry the controls are insufficient, in ways the main study examined does not need to worry about. Neither study has the usefully power to look at deaths, and the death data does not leave room for much effect there. There is still plausibly substantial benefit on preventing lesser injuries.
The first best solution to this particular issue is to eliminate all car seat requirements, or at least limit such requirements to children under the age of two. 
Failing an age two limit, limiting to the first four years would confidently capture most of the safety benefits, while eliminating more than 80% (and likely more than 90%) of the negative effects.
Failing that, we could even exempt any child with two younger siblings (or even two younger children present in the car), for obvious practical reasons, and still most of the negative effects (at least on births) go away while most of the safety benefits clearly remain. 
Car seat requirements also allow us to get an estimate of what impact a direct front-loaded economic subsidy might have on American fertility. I estimate that various incentives will generate or prevent one additional birth per ~$270,000 in effective size, with the note that front-loading likely increases this effect while spreading it out over a longer period will likely decrease it. 
This happens to correspond almost exactly to the cost of raising a child to age 18.
To the extent that there are real safety benefits to car seats for older children, it means the true cost of the mandate is lower, which in turn would lower my estimate of the necessary incentives accordingly. 
Here are the key findings and explanations, starting with the abstract. Anything [in brackets] throughout this post is me editing/clarifying based on other things from the paper. And I add paragraph breaks, because almost all papers need more of those.
Abstract: We show that laws mandating use of child car safety seats significantly reduce birth rates, as many cars cannot fit three child seats in the back seat. Women with two children younger than their state’s age mandate have a lower annual birth probability of 0.73 percentage points [a relative drop of 7.8%]. This effect is limited to third child births, households with access to a car, and households with a male present, where both front seats are likely to be occupied.
 We estimate that these laws prevented [at most] 57 children’s car crash fatalities in 2017, but prevented 8,000 births that year, and 145,000 births since 1980.
[from later]: By contrast, if current laws had applied over the whole sample, we estimate there would have been a further 350,000 fewer births.
…
Our prediction is that third child birth rates are reduced only when the first two children are younger than the state/year mandate, which is itself changing over time.
…
We consider one unexpected cost – child car seat laws. Since 1977, U.S. states passed laws mandating that children be restrained in child safety seats. While initial laws typically applied to children ages one to three, since the mid-1990s mandated age limits have seen a huge ratcheting upwards. The median age at which children are allowed to ride in just a seat belt in U.S. states is now eight, and every single law change has increased the age, not reduced it (with the average state making 3.2 such age changes). 
Enthusiasm for these laws has not been curbed by studies showing that child car seats are no more effective than seat belts in preventing death or serious injury for children above age two (e.g. Levitt 2008, Doyle and Levitt 2010). This may be due to the perception that such mandates are virtually costless, beyond that of the car seats themselves.
However, these laws can significantly raise the cost of another child for women who already have two young children. While the exact type of mandated restraining device varies, many cars cannot easily accommodate three child seats in the back row of seats, as would be needed if both front seats are occupied by adults
…
We find that the estimated effects are driven entirely by households with access to a car, and where there is an adult male in the household, increasing the likelihood that both front seats are occupied by adults. Surprisingly, the effects are larger among higher income households. This suggests that the channel may not be just money, or that these groups bear a greater burden through higher compliance rates, greater knowledge of the law, or being more likely to plan their fertility.
…
[Effect decomposition]: The first represents an intertemporal shift in birth rates, whereby child birth is delayed until existing children age out of car seats, offsetting some or all of the contemporaneous effect. The second component represents a permanent effect, whereby the additional cost dissuades some women from ever having a third child or more.
…
Each additional year that the two eldest children are required to be in car seats results in a 0.60 percentage point reduction in the lifetime probability of having a third child.
…
We estimate that switching from an eight-year-old mandate to a four-year-old mandate would result in the average woman having 0.0076 more children. [1 per 131 women who have two children.]
…
Existing work such as Levitt (2008) and Levitt and Doyle (2010) shows no significant effects of the use of car seats on death or serious injury rates for children over age 2 relative to seat belt use, conditional on getting in a crash. This still leaves open the question of the impact of mandates themselves, whose effect may not map cleanly to actual usage. Using Federal data on fatal car crashes dating back to 1975, we estimate the effect of car seat laws on the number of child fatalities in car crashes to be very small. Our best point estimate corresponds to car seat mandates preventing only 57 fatalities of children below age eight in 2017, with the most favorable estimates being 140 fatalities prevented. In these and most specifications, we cannot reject a null hypothesis of zero lives saved.
…
A surprising aspect of our results is that while the costs of upgrading to an SUV or minivan are non-trivial, they are relatively small compared with the lifetime costs of raising a child. This makes it puzzling that our effects are as large as they are. One possible interpretation here is that the people being influenced are very close to the margin of indifference, and there are a surprisingly large number of families in this situation. However, an alternative is that these costs are unusually salient relative to other considerations because of how immediate they are. If individuals engage in hyperbolic discounting (e.g. Laibson (1997), DellaVigna and Malmendier (2006)), immediate costs like buying a new car even before the child is born may end up weighing more heavily than much larger, far-off costs like paying for college. Another possibility is that the cost of a new car is not so much the actual reason, as much as a rationalization or polite excuse, especially if there is some disagreement between the parents (Voas 2003). The question of why car seats weigh as heavily as they do is an interesting one for future research.
…
Finally [among other similar estimated effect sizes for other things], our estimate is similar to recent literature examining Russian maternity benefits (8.8% from Malkova (2018)) and mortgage deregulation (6% from Hacamo (2020)). From a policy point of view, car seat regulations are unusual in that increasing birth rates does not necessitate greater government expenditure, but simply removing a costly mandate.
Before diving into the details, what might the key takeaways be here, if true?
There are 145,000 births prevented, versus a population of 332 million.
About one American in 1,150 that would have been born since 1980, was never born due to this law. (source for overall births data).
Going forward, given current law, this will likely increase to 1 in ~500.
About one American in 2,500 that would have otherwise been alive today doesn’t exist because of this requirement. 
That’s a lot.
A decline of 7.8% is quite a lot indeed.
Among those fully impacted it’s going to be quite a lot bigger than even that.
It is not clear these laws do anything to stop fatalities for children over 2.
For every fatality prevented, we prevent at least ~140 births. 
We also prevent some number of other injuries.
We also impose a lot of costs on families in various forms.
Many people probably really hate minivans.
Many people care a lot more about immediate costs than long term costs.
These car seat laws that go out to 8 (!!!) years old are flat out nuts.
Allow me to emphasize that last point once again. These laws are completely, utterly nuts. They impose a real and substantial cost on families, which is so large that they forgo having additional children. 
Even when that does not happen, it’s super annoying having to shuffle around car seats between vehicles or between trips. It means that one needs to have a background terror that if one took a taxi or otherwise tried to live life normally that one could somehow get in trouble. It insults the children. It is actively uncomfortable. At least the one car seat we have used was remarkably annoying to buckle and unbuckle, sufficiently so that I tried to avoid using the car even more than I would have otherwise. We now once again live in NYC so we have no car, yet for all three births worried before births if we would be allowed to take our son home without a car seat. The shame factor of having to purchase a minivan, should that arise for someone, is often not small.
I can totally see, as a father of three, how this could prevent births.
There is a claim to substantial safety benefits via older studies. I examine two of them in the Safety section. I don’t see anything large enough to plausibly justify such mandates in places where use is expensive, but it makes the decision to use a car seat for longer seem more reasonable when made by an individual who has that option.
The idea that most seven year olds require a car seat simply makes no physical sense. It is patently absurd. Nor is it a rule that one could remotely hope to enforce.
This can then serve as an example both of of an insane rule that still keeps getting ramped up to higher and higher levels of insanity, and also an example of an easy way to show that yes we can raise birth rates by improving the lives of families. And it can provide a point estimate of how much it might cost to do that with money, where we must do so with money, although this is complicated by people’s hatred of minivans.
Before getting into the study’s true guts: What’s the core problem with the fertility rate? As the paper notes, we know about birth control, about female education and labor force participation rates, and about economic development, and other general factors, but none of those needles have moved in the United States for a while and yet the birth rate keeps declining. 
No one is saying that car seats are the primary motivating factor here. Can economics explain it more generally?
The recent decline is also puzzling under standard economic theories of fertility. A recovering economy ought to have increased the budget set for available children, unless the costs of having children are rising faster than incomes.
Generally, I would propose five things here.
The dollar costs of having children definitely are rising much faster than incomes. Children disproportionally increase costs in education, housing and health care, which are exactly the sectors getting most expensive. 
The lifestyle costs of having children are also rising. Burdens placed upon parents have increased. It is far easier to get into trouble. There are more things you are forced or expected to do that are expensive in various ways. The things most fun about children and childhood are more restricted. There are fewer other parents, especially of multiple children, to coordinate with. 
Baseline career paths and expectations are increasingly incompatible with raising families, both in terms of real and imagined requirements. More time must be spent in school and then in jobs that do not pay enough, and there is more fear of being left behind, and young people are less able to meet, have sex and relationships and get into position to raise a family. 
Meanwhile, we’ve set the expectation that marriage and kids are touchstones that you get after you make it, not something everyone gets to have and that then provides motivation to make it because you have something to protect. Which again plays into not being able to make it, because you’re not up against others with kids. Social rewards for children, especially those beyond the first or second, are greatly reduced, and media roll models hard to find. 
A ‘recovering economy’ does not match the lived experience of most young people, who as far as I can tell are struggling economically.
Also: Recently, it seems a remarkably large number of people are being tricked into not having children by panic over climate change, and being made to feel bad about the idea of having children. Please do not fall into this trap. 
Some people I know aren’t having children, or have even had vasectomies, due to AI risk. Please do not fall into that trap, either.
Or more bluntly:
Kids are more expensive now in dollars.
Kids are more expensive now in time and experiences and offer fewer rewards.
Kids interfere more with your ability to get dollars, and vice versa.
Young people are told to wait on kids until after they have it all figured out.
Young people have been screwed over and do not have it all figured out.
Young people are letting climate change drive them crazy. 
This is mostly (but not entirely) a straightforward economic story. Higher prices, less willingness to pay, less pressure on people to buy. Quantity declines. 
It is also an overdetermined story. Should this make us suspicious? In some ways yes, in some ways no. Certainly it raises the probability at least some of these are not major factors, but it also makes sense that such reasons would be mutually correlated and reinforcing. 
This points to various different possible solutions.
The good news is that the essential goal is ‘make life better, more rewarding, easier and more affordable to live’ which are all great ideas anyway, so most of this is stuff already worth doing for other reasons. 
Kids are expensive so give parents money, or reduce monetary costs. Presumably start with the big three: Health care, housing and education.
Burdens on parents are too large and rewards too small. Fix it. Reduce expectations and legal requirements of supervision and obsession with safety. Understand what risks are big and real and which ones are tiny and mostly fake, like kidnapping by strangers. This is where car seats come in. Don’t stick social services on people without a damn good reason. Allow kids to be kids and experience the chaos of youth, which is better for them anyway. Don’t teach the kids that they are something bad, or that they need to turn their backs on their parents and their traditions. Make it so a child’s future does not depend on an endless string of boxes being checked by discarding the boxes along the way. Let parents raise their children, to the extent feasible, as the parents best see fit. Ensure more places and events are kid friendly. 
Kids interfere with long career and educational development, and a winner-or-long-term-commitment-takes-all working world. Reducing logistical requirements of children will help, as will lowering regulatory barriers to affordable child care. Reducing occupational licensing requirements and educational system gatekeeping, and changing college and graduate education to accommodate family formation, will help as well. So will general shifts from bullshit jobs towards actual production. 
Cultural messaging can be hard to change especially by fiat, but we are not powerless here. The zeitgeist can be shifted. It can also shift when the underlying facts shift. 
It is not so easy to ‘stop screwing kids over’ and this is essentially telling us to make a better country and get our house in order. So, fair, but the most likely key is to focus on defeating rent seeking by the old against the young. Zoning and other restrictions that drive up rent and house prices, tuition combined with price discrimination and educational job requirements that drives them into debt, and the barrier to letting people start their own small businesses, which are by default one’s best choice and more compatible with raising a family.
Give people hope for the future. Climate change takes away people’s hope, so solve climate change to give them their hope back. Do the same for some other issues. Help us celebrate what’s good and getting better, as many things are.
So, again, no one said this would be easy, but the situation is not entirely hopeless. 
A weird dynamic to consider is that birth rates decline as economies develop, but also all the economic principles and observations of people’s choices say one’s willingness to have children declines as one’s economic situation worsens, once you control for someone’s social class and expectations. 
Thus, I can notice this…
…and still expect that making a given person/couple’s financial life improve will in general will increase their birth rate, even if it’s not focused on things related to children, although that focus will certainly help. Part of this, of course, is that the graph does not represent one directional causation.
We obtain state car seat laws using Lexis Nexis StateCapital and NexisUni.
…
We then identify the age at which point 50% of children can ride unrestrained using a seatbelt.
My guess is using straight age restrictions would have been better, because the weight exemptions don’t often bind and mostly are ignored, but they note that this would not have substantially changed the results.
Data on car crashes come from the Fatality Analysis Reporting System, from 1975 to 2018. This database contains information from vehicle crashes in which there is at least one fatality, and includes information on vehicle and passenger characteristics.
America is still very good about some kinds of data collection, which is handy.
Those are easy, fertility conditional on various other things is the tricky one that requires multiple data sources:
Our data on birth rates come from a compilation of assorted U.S. Census Bureau data products, aggregated and standardized by IPUMS USA, a service of the Minnesota Population Center. More specifically, our primary analysis is based on yearly vintages of the American Community Survey (ACS) conducted from 2000 through 2017, and 5% random samples from the 1990 and 2000 decennial census.
The result is a dataset comprised of repeated cross-sectional snapshots of U.S. households taken at different points in time. For each cross-section, we are able to observe key characteristics of all surveyed households, including the age and sex of the household head, all other adults (including spouses), and all children present in the household. Moreover, each survey includes other relevant characteristics, such as race/ethnicity, household income, car ownership, and geographic location (generally at the county level, though at the state level in earlier vintages).
This suggests that we have a bunch of other useful data that can provide color on the situation. Car ownership being included seems quite useful even if it doesn’t directly impact the headline answer.
Sorting everything gets a little odd.
We are interested in the decision of each woman to give birth, rather than each household. We exclude group homes, households with inmates or children-in-law, and households with no adult woman present (as households with only an adult male and children will lack information on the mother’s age). If a household has multiple women over 18 years of age, we split them into separate observations, assigning children to the corresponding mother, and noting the presence of any adult males. This unit, which we term a “household”, thus differs from the census definition. 
We then take information from the survey year t for the age of the woman and any children, and infer ages and birth events for all prior years from t-1 back to the year in which the adult-age woman was 18 years of age.6 In doing so, we are only able to infer birth events for children that remain in the household until the time the snapshot is recorded.
In practice this is measuring the thing we care about. A child no longer in the household presumably won’t matter (or won’t matter much) for the purposes of car seats.
This is at least a minor issue:
We implicitly assume that a household has not moved across states from when the woman was adult-age until the year of the snapshot.
Moves that are for unrelated reasons will tend to scramble the data and make car seat impacts look smaller than they are - paper thinks the effect is unclear but that seems wrong to me. Moves that are party about car seat rules or other related rules would likely do the opposite. Moves intentionally towards permissive areas would be strong arguments for permissiveness. Moves intentionally towards restrictiveness would be the opposite but I am trying to imagine ‘I am going to move to where others have to use car seats for longer’ and my brain can’t do it. Paper mentions that strategic moving seems unlikely. 
Result is a very large sample. It is missing substantial numbers of births…
In Panel B, the final panel has approximately 69.7M household-year observations with an average adult woman age of 25.3. The average woman in our sample has an annual probability of giving birth of 8.4%. In comparison, a back of the envelope estimate of publicly reported fertility rates for women aged 18 through 35 in 2010 is 9.1%. 
One contributing factor to our lower estimated birth rate is the inability to observe births in which the child was subsequently separated from the mother (e.g., through adoption, divorce with fraternal custody, etc.). However, it is unlikely that this lower estimated birth rate poses a significant challenge to our empirical strategy, which is largely based on both state and year variation in car seat laws, and variation in the number and ages of children across households in a given state.
…but I agree with the paper that this should not impact the approach taken.
Non-linear effects from increasing ages, as you’d expect (italics in original):
For instance, mandating use of car seats until age 4 (as many early laws required) will affect relatively few women, and for a relatively short period of time. In particular, for a woman to be impacted, she would need to have given birth to two children who are both age 3 or less, and be looking to give birth to a third before the oldest reaches age 4. If each pregnancy takes roughly 9 months from conception to birth, three children require at least 27 months, even in the unlikely scenario of no gaps between birth and conception. With a 48-month mandate, and 27 months of pregnancy time, the only women affected would be those who wished to get pregnant again in less than 10.5 months on average after their first and second children (or those who had twins or other multiple birth events). In other words, the practical burden of low age mandates is likely to be very small. 
At the other end of the scale, increasing ages to 6 or 8 is likely to have much bigger impacts. The likely spacing of births every couple of years will see a larger fraction of women already affected by the mandate, and extending it further will cut into periods when additional births might otherwise be likely.
To see this effect, in Figure 2 we plot the fraction of person/year observations in each year that are impacted by car seat laws under our definition– that is, women who have two children that are both legally required to be in a car seat. The graph shows that the impact was almost zero until 1983, around 1% of observations affected between 1984 and the early 1990s, increasing slightly to around 1.5% of observations in 2000. However, there was an enormous increase between 2000 and 2010, to over 6% of observations affected. In other words, while the existence of these laws is quite old, the period where they have had significant bite is relatively recent.
…
In particular, what we require is that having a third child is discontinuously more difficult if both existing children are mandated to be in car seats.
Bingo, and a big reason the policymakers did not understand what they were doing. I am guessing they never thought about the situation where there were three children that all required car seats and that they would not physically fit in the same regular car, whereas three children can otherwise sit in the back without problems. All right, not with zero problems (have you met children?) but nothing you can’t handle.
Also, note that this table shows how the number of years the laws bind impacts the probability of ever having a third child (ignoring effects on fourth and later births, and it is clear as day.
(The numbers in parenthesis are t-values.)
Eight years bound meant twins, which shows a huge jump. The effect here is highly non-linear, on top of the non-linear impact on years bound from the laws. It seems clear that a four-year law has less than 10% of the impact of an eight-year law - compare one year to five, and two to six. 
The paper also notes that the presumed mechanism whereby car seat requirements prevent births sometimes will not apply. Not every existing car will be able to fit three children in the back with two car seats. Many will be able to fit three car seats already, without upsizing:
But notably we do not require that it is impossible to fit three car seats in the back of all cars, nor that it is straightforward to fit in two car seats and a third older child in between in all cars. There may be some small cars whereby fitting in an older child in the middle may still be a struggle, which would make the law less binding at older ages (because the same cost of a bigger car is imposed even if the oldest child has aged out of the law). 
In addition, there will be some women with larger cars who have access to modern smaller booster seats for older children, who will be able to have a third child without needing a bigger car, even if all three are bound by the law. For our tests to work, there only has to be some significant fraction of people who are affected in a discontinuous manner by these changes, which we feel is likely. 
If these discontinuous costs mean that some sizable fraction of the populace will need a larger car for a third child, this is predicted to reduce third child birth rates.
They frame this as ‘the effect only needs to apply sometimes’ but it is worth noting that this means that when the effect does show up, its impact will be proportionally larger than measured by the whole group. If half of parents already have a minivan or other sufficiently large car, or would have to upgrade anyway, or don’t own a car at all - hello, New York City - then they presumably aren’t being impacted by such laws, which would mean that the other half is seeing twice the decline that the overall average shows.
What do the people tell you?
Anecdotally, it is rare to find people who will openly state that car seats concretely stopped them having a third child. But it is quite common to find people who cite car seats as being a pain and a hassle that they had to deal with, especially in the context of having a third child.
That’s why we check the stats and revealed preferences. If a family did decide to not have a third child due to a car seat requirement, I would not expect them to tell you this when asked, even if they consciously knew it themselves. There are lots of good reasons to have or not have another child, so it is easy to come up with an answer that plays better on all levels than ‘I didn’t want to buy or couldn’t afford a minivan.’ 
To identify the effect of car seat laws on fertility rates, using our panel of female-year birth outcomes we estimate OLS regressions of the following form:
Which is a lot of Greek symbols that one does not need to worry about, it’s another way of saying ‘we did a regression on a bunch of factors.’ It effectively results in:
I am not checking their math and regressions, but in terms of methodology this all seems fine and standard. 
What was found? They describe Panel A. 
Women with exactly two children both under the state-year car seat mandate age have a probability of giving birth that is lower by 0.422 percentage points (t-statistic of -2.79). In terms of economic magnitude, by way of comparison over the whole sample (years from 1973 to 2017), the likelihood of giving birth in a given year for women that have exactly two children is 12.14% for 25-year-olds, 8.55% for 30-year-olds, and 5.20% for 35- year-olds. 
Column 2 adds controls for the woman’s age and race, and the effect increases to -0.554, with a t-statistic of -4.09. 
Column 3 adds ex-post demographic controls measured in the survey year, namely household income, education, and male type (i.e. husband, permanent partner / other adult male, or none). The effect increases slightly to -0.620, with a t-statistic of -4.80. Columns 4 through 6 allow all demographic controls to vary by year, county, and both county and year respectively. 
The effect increases in the full controls specification to -0.732, with a t-statistic of  -5.69.
The full effect size comes after putting in these various controls. I do not have a good intuition for why the controls should increase the effect size this much, but the majority of the effect is present without any controls. 
The above covers women up to age 35. They then extend this analysis to ages 36, 38 and 40, which they note increases measurement error, and get -0.756, -0.699 and -0.688 respectively, which shows the cutoff at 35 wasn’t important. Women ages 36-40 have fewer third kids so the overall size dropping there is expected.
This test does seem to have potentially turned up something:
Finally, column 5 includes only observations from the 2000 ACS onwards. This sample ensures that at every date we have the full sample of female ages from 18- 35, albeit at the cost of examining a considering shorter time period. The effect of Two Children, Both Bound is -0.476, with a t-statistic of -3.42.
Panel B shows the effect size seeming to decrease somewhat over time. 
That’s a substantially smaller effect for the later time period. There is some possibility that impact of similar laws is declining over time. Possible hypothesis is this happens over time as cars become relatively better and more affordable and people’s wealth increases. Also fertility declined in the later period, which will make the effect look smaller, and accounts for a decent chunk of this.
The conservative thing to do is plausibly to stick to this later-stage point estimate of -0.476 percentage points per year, rather than -0.732, if one wanted to know what size effect one could count on going forwards. 
They then consider some other possible error sources, none of which seem to me to be worth worrying about given the full context, and dismiss them. 
Next the paper finds strong confirmation of the causal theory. In cases where the car seat constraint binds less - either no car is owned, or no male is present so the front seat is free - the decline in fertility vanishes. 
They then find the effect modestly increases with household income. They speculate, as I would, that this is a combination of more value on class signals and more reluctance to defy stupid state mandates, but that is only speculation.
Urban density also modestly increases the effect, for unclear reasons.
In terms of showing that births were reduced, conditional on the data and math checking out, I find this very convincing. This looks and feels real. 
The paper then considers the possibility that children are sometimes postponed until conditions change, but that the permanent effect size could be smaller. 
Their conclusion is that there is some of this effect, and this reduces the temporary net effect from 0.72% to a permanent effect of 0.6% per year. 
If we look back at the chart labeled Panel B above, we see that going from a four-year law with a newborn and a two-year old to an eight-year law changes the impact of the law from 0.32% reduced lifetime probability of a child to a full 4.48% reduction. So there is over a 4% chance that this changes final family size from three or more children down to two. That includes households that would never have had a third child anyway, and in many cases a family might still have a third child but be shut out of a fourth or fifth because they run out of time, so this is a huge reduction in future births.
It seems right to care more about permanent births than temporary births, but the difference between 0.72% and 0.6% could easily be made up for by the decline in fourth births in the future. Very few of these births effectively get ‘made back up’ later. 
This checks with the many observations that families often want more children than they end up having, and the many cases where biology refuses to cooperate. Also, once you have a large gap in time, it is more tempting to stop and ‘get your life back.’ The clock, in many ways, is always ticking.
When they do a simulation of the entire population, they get this result for all women:
Figure 3 Panel A shows that the average woman in the sample would have a higher probability of giving birth of 0.012% per year for uniform mandates at age two, and 0.0096% for a four-year uniform mandate. The effect on birth rates is fairly similar within this range.
…
However, as mandates increase to age five and upwards, the estimated average birth probability decreases considerably. A uniform eight-year-old mandate would result in a lower annual birth probability per woman of -0.033%, and a uniform nine-year-old mandate would result in lower annual birth probabilities of -0.046%.
…
The difference between a uniform four-year-old mandate (0.0096% relative to existing laws) and a uniform eight-year-old mandate (-0.0325% relative effect) is a 0.042% per year difference in average birth rates.
This is why I can say with confidence (as I do in the overview) only that more than 80% of the negative effects can be eliminated, rather than the 90% that seemed to follow from other numbers elsewhere. 
We estimate that uniform age mandates of two, three and four years old if applied throughout the sample would have led the U.S. to have had 145,000, 143,000 and 117,000 more births, respectively. By contrast, seven- and eight-year-old mandates would have resulted in 235,000 fewer and 392,000 fewer births respectively. In other words, a switch from a uniform three-year mandates to uniform eight-year mandates would have been associated with 536,000 more births.
…
Another useful counterfactual is to estimate how many fewer births would have occurred if the heterogeneous laws in 2019 had been enacted across states in 1980 and not changed since. This would have resulted in 350,000 fewer births.
…
A uniform four-year-old mandate in 2017 would have resulted in 7,700 more births in 2017, and a three-year-old mandate would have resulted in 8,000 more births.
Next, a quick survey to confirm that yes, children impact and force car purchases:
Multiplying the 51% of parents changing cars around the birth of children, and the 38% of them citing car seats as being important in this decision, gives 20% of surveyed parents changing their car around the birth of a child with car seats being a large or a very large factor in their choice. These results support the viewpoint that fitting in child car seats can pose a significant potential cost when having more children.
A common argument for minimizing the impact of such rules is to point out that there technically exist ways to cope with them. This post on LessWrong suggests sufficiently small boosters exist. This would still be highly annoying, and intuition says that this is going to be a very tight squeeze if the kids are approaching eight years old even in the best case.
If you are in this spot, I do think it is worth checking if such solutions work for you.
What is important to the policy question, however, is that, even if such a possibility exists, it does not negate the practical impact of the rules on fertility. If someone does not consider an option, then for the purposes of their decisions it does not exist.
Do car seats, and car seat requirements, improve safety?
Existing work on the intensive margin of using a car seat (e.g. Levitt 2008) finds minimal effects. However, these papers largely examine the impact of using a car seat, whereas the impact of mandating safety seats is plausibly even lower.
How polite. The impact of mandating them is going to be far lower, because some parents will use them anyway and others will ignore the mandate. 
If car seats are worth using, mostly we should expect to see them in use, because parents care about the safety of their children. If anything, modern parents care too much about improving the marginal safety of their children versus other things children need. I would expect overuse of car seats even without a mandate.
What did Levitt find? From the abstract:
We find no apparent difference in the two most serious injury categories for children in child safety seats versus lap-and-shoulder belts. 
Child safety seats provide a statistically significant 25% reduction in the least serious injury category. 
Lap belts are somewhat less effective than the two other types of restraints but far superior to riding unrestrained. (JEL I18)
It seems physically plausible to me that the seats could prevent some minor injuries while not substantially impacting serious injuries and fatalities.
The new paper does a new analysis, and finds this:
Overall, the most reliable effects are observed for restrictions on four-year-olds and five-year-olds, in terms of being consistently negative point estimates across specifications, and showing some marginally significant effects with the full set of controls. These estimates indicate a reduction of -0.695 deaths per 100,000 for four-year olds (t-statistic of -1.79), and a reduction of -0.762 deaths for five-year olds (t-statistic of -1.84) (-0.598 deaths and -0.871 deaths respectively in column 8 with only eight-year olds and below included). Meanwhile, there are some ages (such as one-year olds) that show significantly positive effects on death rates from car seat restrictions, an effect for which we do not have a clear explanation.
The whole thing is a mess. I think we are meant to focus on the fourth column in each group. It seems non-obvious that car seats are doing net-positive things. The most generous interpretation here has the seats being net safer up through year five. 
But what does this look like most? The null hypothesis, where the seats are doing nothing and the whole thing is random. Which was what Levitt found as well. 
The paper still asks, what if they are as real as they could plausibly be?
While the evidence for car seat laws reducing death rates is weak and inconsistent, our main interest is in the economic magnitude implied by the point estimates. We multiply the coefficients in Panel B by the total number of children who are restricted in all U.S. states in 2017 (the year with the most stringent laws in our sample).
In the full specification of fixed effects (column 4 and 8), the number of children’s lives saved nationwide is estimated to be 40 in column 4 and 57 in column 8. Under the most charitable interpretation where we take the maximum effects and treat all negative point estimates as being genuine but all positive point estimates as being zero, the total number of lives saved is estimated to be 122 and 140 respectively. 
To give context, it is worth noting how many children’s car crash fatalities there are in the first place in the U.S. In 1978, the number of fatalities for children age 0-8 was 2,392. By 2017, this had declined 73% to 654 total. 
However, this is almost exactly the same as the 72% decline in fatalities for children ages 9-12 over the same period from 975 to 273 (who were largely unaffected by these laws). 
In other words, back of the envelope approximations are consistent with the small numbers we document.
If fatalities declined the same amount for both age groups, that once again strongly implies the null hypothesis. At minimum, most of the gains here are clearly from other sources, and any benefits from safety seats has declined by ~70%. 
What about the study that claims that there is a reduction in injuries up to age 12 here?
Before I looked, I had a simple hypothesis that this would be driven by common cause. If you choose to use a booster seat for your ten year old, you are not taking a normal attitude to safety. You are going to be driving slower and safer, and in safer roads, in safer cars, and so on. 
Thus, the first thing I looked for was, did the study control for those factors?
There was control for model year of car, and general car type, and positioning within the car, and conditions of the accident like precipitation, weekend (?) and so on.
Since we care most about incapacitating or fatal injuries I looked there first. They found no effect unadjusted, a large effect adjusted (which is an absurd claim if taken seriously) and  error bars here are absurd to the point where we can safely ignore the results entirely rather than pick them apart more. 
For any injury at all, they have unadjusted risk ratio 0.709 and adjusted of 0.814 [0.749,0.884], so the adjustments are now in the intuitive direction. They have ages 8-9 with a ratio of 0.869 [0.818,0.923] and the 10-12 year olds at 0.675 [0.505,0.902]. 
The 10-12 year old CI is presumably super wide because very few kids still wore car seats at those ages.
That means that the combined group’s result, where the ratio should be closer to 1 and which they claim is their main result, is outside (barely) the 95% confidence interval for the 8-9 year old group. 
They mention that child height and weight are factors in whether to use a booster seat and are not considered. If I am understanding the dynamics correctly this should reduce effect size some amount, but my guess is the real factor is ‘am I obsessed with safety or not?’ Which, again, means those parents are most likely to use them correctly, and to take hard to observe other safety precautions. 
I am willing to believe that the effect here constitutes a reduction in minor injuries, but the size here is at best a very hard upper bound on the effect size, and the other studies show that car seats in another group had a larger than this effect on minor injury with no detectable effect on major injury. So I don’t see how the effect on incapacitating or deadly injury, which are the only place where the economic effect could be big enough to matter, could be large. Of course, that’s for the 8-12 age group, so that result is no surprise.
The other study is in ages 4-8, so more on point. It rhetorically assumes its conclusion. It says previous research demonstrates reduction in injury in ages 4-7 by 59%. This is the kind of research that makes claims about ages 4-7 ‘based primarily on children who were ages 4 and 5.’ This was then used to impose such rules on children up to age 8. Clever trick. 
The data controlled for a few things, similar to the other study, but seems to be less thorough about it. That is confirmed by a smaller adjust, here from a 49% odds ratio to 55%. This could also be explained by the drivers here being far less weird, since many were only obeying a legal requirement and making a more plausible choice aside from that. Injury was defined as AIS score of 2+. 
There is no question that this result looks substantial and robust as far as it goes. It does not include any measures of death, presumably because deaths were too rare.
Another intuition pump: The CDC says seat belts reduce deaths in adults by 45% and injuries by 50%. The booster seats are used in order to mimic the ‘proper’ effects of such safety belts. A ‘belt positioning booster seat’ having a larger effect than this when compared to an adult safety belt seems, shall we say, like a highly implausible result.  
The population-wide mandate-dependent statistics seem to me to be more robust on several levels, especially in terms of the dangers of potential confounders. 
Another population-wide check that makes sense: Look at children and compare to the general population. 
Since 1975, fatality rates dropped 77 percent for infants, 75 percent for children ages 1-3, 52 percent for children ages 4-8, and 49 percent for children ages 9-12. 
For passenger occupied vehicles and kids younger than 13, we see a steady decline from 1,384 deaths in 1975 to 612 deaths in 2019. We see a much larger decline in pedestrians and bicyclists killed. 
Population sizes were similar throughout. 
If child seat use increased safety by a lot in ages 4-8, you’d expect to see it show up here when compared to ages 9-12, for whom use of such seats remains rare and mostly not recommended. Looking at the chart, you can see that nothing much changed here in the 4-8 vs. 9-12 comparison, but for the 0-3 year old groups things did improve in relative terms.
This again strongly suggests that the benefits are concentrated in the 0-3 age range, and the rest of the gain is coming from other sources. 
(We see a decline in motor vehicle deaths overall, but it is smaller. I am guessing this is mostly due to demographic shift - Americans are getting older, and older people die more often in crashes.)
The bottom line is thus: In a counterfactual world, where the mandates end at age 4, what would you expect the 4-8 year old column to look like instead? There simply isn’t any room left for there to be a big safety impact that could possibly make these laws make sense, even if maximal estimates are used. 
If we make reasonable assumptions of declining marginal safety as kids age and that parents will use the seats the first two years regardless, and then look at how much impact you can mathematically fit in here, the answer is very little. 
A question left unanswered is why such policies have been adopted almost universally across states and grown steadily more stringent, even without Federal mandates (Bae et al. 2014). 
The answer that seems most compelling, but perhaps surprising under public choice economics, is that regulators are simply unaware of the magnitude (or maybe even the existence) of costs being imposed on family formation. At a minimum, the relative lack of public discussion of this tradeoff suggests that it may not be foremost in the minds of policymakers. It is nonetheless difficult to imagine the compelling social interest in existing policy arrangements.
Hahahahahaha. Yes. I am sure the regulators involved are merely unaware of the issue and that they will come around as soon as it is pointed out to them.
No.
To be clear, I don’t think that the regulators had any idea whatsoever about this effect. 
That’s because they don’t care. 
Such costs are not their problem in any way. Their goals and cares lie elsewhere. They don’t do a cost-benefit analysis. 
They don’t think about or talk to a normal human in the situation they are regulating. Or consider the physical layout of the car of that normal human and whether the things in question will fit into that car and how much that might cost that human. 
They also choose not to notice that no one has ever run a proper control for the crash test dummy experiments used to justify car seats, except that one time that they found the same results for those dummies that didn’t use a car seat. You see, when you ask to do such tests, the testing companies refuse, because they don’t want to piss off their customers. 
This is not an innocent mistake. The magnitude of the damage was likely unintentional, but the idea that all of this was the result of well-intentioned folks all around? No.
This is not complicated. It is a clear case of ‘who benefits?’ with a clear answer: The regulators and lawmakers get more authority and power and get to use it and look like they care. The car seat makers get to sell more officially approved car seats. The gas companies get to sell more gas because the new vehicles are less fuel efficient. The automobile manufacturers get to sell more minivans. 
Meanwhile, the usual Public Health Concerns get trotted out, to make it clear that an imagined possible improvement of any size justifies any amount of cost and any number of other consequences. There are so many hot takes. The first comment on the podcast link above is that children are bad. The second is that the dummy tests aren’t representative (and presumably somehow neither are the accident statistics). 
The fourth literally says this:
I'm a pediatrician
…
Decreasing severe head trauma even by 0.001% is more than enough to justify the &quot;cost&quot; of the car seat or inconvenience. The economists act like a small reduction in head trauma is not worth the cost of having to install the car seat, but anybody who has treated pediatric victims with brain trauma from car accidents would tell you that's insane!
For those who are curious about the offering here: Google says there are about 50,000 such traumas each year, from all sources, so 0.001% even from all sources means one such injury prevented every two years. 
The sixth comment once again calls upon us to celebrate the lack of births.
The seventh confirms that the minivan issue was a factor in a couple’s decision.
After that, it gets worse.
The better question than asking ‘why’ here is, ‘why not?’ Who is going to stop and reverse this? And then hopefully open the door to targeting other child-unfriendly, family-unfriendly needless regulation, ‘for our own safety’ or otherwise? 
To bring it all home, we must ask what the true cost of the car seat policy is. 
Consider the car seat requirement as decomposing into a few different consequences.
The need to buy a new car, presumably a minivan ($$$).
The need to fill up that minivan ($$$), since it’s going to be less fuel efficient.
The need to use car seats that aren’t needed, since they’re annoying.
The need to drive that new minivan, including social downsides.
If you buy a new minivan, MSRP is likely about $33,000. 
If you buy a used minivan, it will be cheaper. A very quick survey said maybe $15,000.
Let’s say on average this net costs $15,000 after your trade-in. 
For gas mileage, you’re likely talking about an average of a few miles a gallon, something like 20 instead of 25. The average American drives about 15,000 miles per year, so this is about 125 gallons a year, historically let’s say $3/gallon so $425/year for a few years, so this cost is relatively small, a few extra thousand.
How much direct disutility from using the car seats? If you assume the average child on the margin is going to school and camp, one can guess something like two trips per day, which means getting in and out of the car 730 times a year. How much would you pay each time to not have to deal with that stupid car seat? How much would you pay to not deal with the middle car seat, in particular? This is arguably a bigger cost than the gasoline for some, but mostly let’s assume it’s not so bad. 
How much worse is it to be forced to drive a minivan? I did a quick unscientific poll.
This seems to safely say the dollars are a bigger deal, but 41% thought the non-dollar concerns were larger, so they don’t dominate by much. If we interpret almost all as 100/0 and majority as 66/33, we get about a third of costs from non-$ factors. That seems sane.
So let’s say that it costs $18,000 all-in for the van and extra gas, and another $9,000 in various anguish for the minivan. If we round up for the actual car seats themselves, we can say roughly $30,000 all-in for those in this situation. 
Obviously, the true costs will vary a lot. Some will already own such a car, or not need one, or have been planning to upgrade anyway, and thus will be down $0. Some will kind of like getting a new bigger vehicle. Others will really, really loathe their new minivans. 
91.5% of all households own a car. Let’s guess that 80% of households in this position would need to change the car they drive, with 11.5% already owning a sufficient car (or that would have upgraded anyway) and 8.5% who don’t have a car period.
Those in this the situation ‘have two children who require car seats and no other children’ reduce their yearly chance of birth by 0.73 points, or a relative drop of 7.8%, when this cost is imposed on them on the margin. 
That means that of those where the constraint binds, there is a relative yearly drop of 9.75%, or an absolute decline of 0.91 points. 
So approximately we find that a tax of $30,000 cuts fertility by 10%. If this was purely an actual tax, then we would be collecting $270,000 in taxes for every birth prevented.
It is worth noticing that the bulk of this tax is due immediately at birth. This is about 2/3rds an up front cost. Up front costs likely have a lot more impact than longer term costs, even factoring in a reasonable discount rate. 
We don’t know that a subsidy would have the same effect, as it might be viewed differently, but a reduction in costs in other ways likely would. And some people would greatly benefit from a large subsidy, so it isn’t totally clear the net impact would be smaller. It’s hard to know for sure.
If this were a matter of ‘the money you spend to induce births results in that much real value being destroyed in a pit and the money will be gone for no benefit’ then $270,000 is a lot of money. 
As an interesting point of comparison: How much does it cost to have a baby and raise them? A 2015 study done by the USDA found that it cost an estimated $233,610 to raise each of two children from birth to age 17 in a middle-income family with two parents. That figure, adjusted for inflation, is just over a quarter of a million dollars at $286,000 in 2022.
In other words, the cost to get families to have another child is almost exactly the same as the cost to raise a child to age 18. That’s a coincidence, but it creates quite the intuition pump: You pay the costs of raising the child, and in exchange the parents agree to have it. 
A second point of comparison: The average American will pay $480,407 in taxes throughout their lifetime. That's an average of 34.49% of all lifetime earnings spent on taxes. So you’d be spending ~60% of the expected tax revenue up front, and only getting it back over time, and people require a lot of services on the margin. Not obviously a great deal.
Another less serious but more fun potential comparison point: Aella put her proposal of ‘have and raise your child’ at $1 million per baby. Offer likely still open.
In another sense of expensive, whenever there is an obviously crazy and stupid legal requirement, it reduces respect for the law and trust in our government and authorities generally. This is usually considered a cost, but can also be a benefit. In general, teaching people to resent and whenever possible ignore the law, and that the law should not be expected to make sense, should be considered a cost when considering what to do, even if this is often the sane reaction to current conditions. 
The good news is, that’s not how any of this works. The money is not destroyed, as it is when families are forced to suffer for no reason. This would effectively be a shift of the tax burden away from families to households and individuals without children. The money is transferred, and can be used by families to buy goods and services like food and housing and childcare. 
There are plenty of good existing reasons to want to do this transfer and provide a child tax credit. The child tax credit enacted by Congress during the pandemic severely reduced child poverty, generally making kids importantly better off, and still didn’t come close to covering the costs of raising children. It was good policy all around, even without a birth rate impact, and if anything should have been longer term and larger, and more front-loaded because the costs of children are front-loaded and also front-loaded payments have more impact on decisions than spread out payments.
I am guessing that is a lot of why previous attempts at subsidy in various places didn’t have as much impact as their proponents were hoping to get. The payments have been too spread out. Doing things like Hungary’s ‘have enough children and you are then permanently immune from income tax’ is a very long-term proposition, among its other problems. 
The obvious direct step is to eliminate any and all car seat requirements beyond two years of age, or at least within three or four years of age. The vast majority of the damage done by car seat mandates happens after age four and almost all of it happens after age three, whereas the vast majority of the safety benefits (or, more likely, all of them) happen by age two and certainly by age four. 
While we’re at it, let’s end this equally absurd rule that minors are not allowed to ride in the front seat. On a recent trip, a friend of mine was pulled over and her thirteen year old daughter, who is taller than she is, was told she had to move to the back seat because minors cannot ride in the front seat. I am going to go ahead and not explain why this rule is absurd, although I will take joy in pointing out that there is consensus we should let some minors drive cars despite them being statistically terrible at doing so, and knowing they will thereby get into a lot of accidents.
More broadly, we can extend the logic of the car seat mandates, in two ways.
We can ask what other rules impose large effective costs on families without much benefit, especially those that come out of an obsession with safety. And especially those that interfere with the logistics of life. Making parents unable to be confident letting their children stay home on their own, or run errands or go to the playground, until in many cases they are in high school, is a severe cost that compounds with larger family size. It is expensive in dollars, in time, in stress and experiences and in the joys of childhood. A startlingly large number of comedians routines include a nostalgia for such childhood experiences, and other ways in which we were previously allowed to handle adversity and risk.  
By removing those restrictions, we can greatly reduce the burden on parents, increase the resilience and joy of our children, at only very small ‘safety’ risk. 
The more ways we find to make modern life compatible with family life and general human existence, the more willing parents will be to have more children, and the better off everyone involved will be on all levels.
The second way we can build upon these results is that we now have a point estimate for the cost of raising fertility via direct adjustment of taxes or subsidies - for every $270,000 or so, about the 18-year cost of raising a child, we induce or prevent one additional birth. This is an additional benefit of transfers to families, which already have many other benefits. 
It also provides an argument for front-loading as many of those payments as possible. This is not studied directly here but it is highly likely that fertility effects are larger on up-front (at-birth) payments than longer term payments, including for reasons of certainty and liquidity and tangibility. One can certainly take this too far, but given the large up-front costs of a child, we are not doing enough to front-load benefits.
Nice work. I read the headline expecting to roll my eyes at the whole thing, but I am convinced that the association is real. Though, I'm not entirely convinced that the survey data can support a causal model, it is suggestive.
Unfortunately, the genie is out of the bottle here. It would be political malpractice to liberalize these safety rules. The first child who dies or is critically injured after eliminating the post two year old requirements is a political disaster for whoever changed the rules.
No posts
Ready for more?</description>
      <pubDate>Sun, 23 Apr 2023 17:53:57 GMT</pubDate>
      <enclosure url="https://JonathanGrant.github.io/podcasts/audio/thezvi_on_car_seats_as_contraception.mpeg" type="audio/mpeg" length="1583179"/>
    </item>
    <item>
      <title>LW Rob Bensinger The basic reasons I expect AGI ruin</title>
      <description>I've been citing AGI Ruin: A List of Lethalities to explain why the situation with AI looks lethally dangerous to me. But that post is relatively long, and emphasizes specific open technical problems over &quot;the basics&quot;.
Here are 10 things I'd focus on if I were giving &quot;the basics&quot; on why I'm so worried:[1]
1. General intelligence is very powerful, and once we can build it at all, STEM-capable artificial general intelligence (AGI) is likely to vastly outperform human intelligence immediately (or very quickly).
When I say &quot;general intelligence&quot;, I'm usually thinking about &quot;whatever it is that lets human brains do astrophysics, category theory, etc. even though our brains evolved under literally zero selection pressure to solve astrophysics or category theory problems&quot;.
It's possible that we should already be thinking of GPT-4 as &quot;AGI&quot; on some definitions, so to be clear about the threshold of generality I have in mind, I'll specifically talk about &quot;STEM-level AGI&quot;, though I expect such systems to be good at non-STEM tasks too.</description>
      <pubDate>Sun, 23 Apr 2023 18:05:15 GMT</pubDate>
      <enclosure url="https://JonathanGrant.github.io/podcasts/audio/lw_rob_bensinger_the_basic_reasons_i_expect_agi_ruin.mpeg" type="audio/mpeg" length="845133"/>
    </item>
    <item>
      <title>Why Checkpoint is Well Positioned to be a Leader in Cybersecurity in the Age of Artificial Intelligence</title>
      <description>1. Introduction to Cybersecurity in the Age of Artificial Intelligence
2. Evolution of Cybersecurity Threats in the Age of Artificial Intelligence
3. The Role of Artificial Intelligence in Cybersecurity
4. Overview of Checkpoint's Cybersecurity Offerings
5. Integration of Artificial Intelligence in Checkpoint's Security Solutions
6. Advantages of Checkpoint's Threat Prevention Architecture
7. Checkpoint's Collaborative Approach to Cybersecurity
8. Checkpoint's Leadership in Cloud Security
9. Customer Success Stories with Checkpoint
10. Conclusion and Future Outlook for Checkpoint in Cybersecurity Industry.</description>
      <pubDate>Fri, 28 Apr 2023 18:56:46 GMT</pubDate>
      <enclosure url="https://JonathanGrant.github.io/podcasts/audio/why_checkpoint_is_well_positioned_to_be_a_leader_in_cybersecurity_in_the_age_of_artificial_intelligence.mpeg" type="audio/mpeg" length="8586604"/>
    </item>
    <item>
      <title>Why Checkpoint is Well Positioned to be a Leader in Cybersecurity in the Age of Artificial Intelligence (Shorter)</title>
      <description>1. The evolution of cyber threats and how AI can help
2. Checkpoint's history and expertise in cybersecurity 
3. Checkpoint's use of AI and machine learning in their security solutions 
4. Checkpoint's partnerships and collaborations for improved cybersecurity 
5. Future challenges and opportunities in the intersection of cybersecurity and AI</description>
      <pubDate>Sat, 29 Apr 2023 20:38:36 GMT</pubDate>
      <enclosure url="https://JonathanGrant.github.io/podcasts/audio/why_checkpoint_is_well_positioned_to_be_a_leader_in_cybersecurity_in_the_age_of_artificial_intelligence_(shorter).mpeg" type="audio/mpeg" length="4529033"/>
    </item>
    <item>
      <title>How AI Will Change the Landscape of Cyber Security and Why Checkpoint Software is Likely to Emerge as a Leader of This New Landscape</title>
      <description>1. The Growing Importance of AI in Cyber Security
2. AI in Cyber Security - Benefits and Challenges 
3. The Role of Checkpoint Software in AI-Enabled Cyber Security 
4. Checkpoint's AI-Enabled Cyber Security Solutions 
5. Future of AI-Enabled Cyber Security and Checkpoint's Potential as a Leader</description>
      <pubDate>Sat, 29 Apr 2023 23:26:37 GMT</pubDate>
      <enclosure url="https://JonathanGrant.github.io/podcasts/audio/how_ai_will_change_the_landscape_of_cyber_security_and_why_checkpoint_software_is_likely_to_emerge_as_a_leader_of_this_new_landscape.mpeg" type="audio/mpeg" length="3666225"/>
    </item>
    <item>
      <title>GPT4 V2 How AI Will Change the Landscape of Cyber Security and Why Checkpoint Software is Likely to Emerge as a Leader of This New Landscape</title>
      <description>1. Introduction to AI's role in Cyber Security
2. Evolution of Cyber Threats and the need for AI-based solutions
3. Applications of AI and Machine Learning in Cyber Security
4. Checkpoint Software's innovative approach in AI-driven Cyber Security
5. The future of Cyber Security and Checkpoint Software's potential leadership role</description>
      <pubDate>Thu, 04 May 2023 18:08:56 GMT</pubDate>
      <enclosure url="https://JonathanGrant.github.io/podcasts/audio/gpt4_v2_how_ai_will_change_the_landscape_of_cyber_security_and_why_checkpoint_software_is_likely_to_emerge_as_a_leader_of_this_new_landscape.mpeg" type="audio/mpeg" length="6257917"/>
    </item>
    <item>
      <title>GPT4 V1 How AI Will Change the Landscape of Computer Peripherals and Why Logitech is Likely to Emerge as a Leader of This New Landscape</title>
      <description>1. A Brief History of Computer Peripherals and the Role of Logitech
2. The Emergence of AI and Its Impact on Technology Development
3. AI-Driven Innovations in Computer Peripheral Design and Functionality
4. Logitech's Investment and Advancements in AI Technologies
5. Future of Computer Peripherals and Logitech's Role in Shaping the Industry</description>
      <pubDate>Mon, 22 May 2023 08:08:56 GMT</pubDate>
      <enclosure url="https://JonathanGrant.github.io/podcasts/audio/logitech.mpeg" type="audio/mpeg" length="7063074"/>
    </item>
    <item>
      <title>Logitechs $100B Plan to Use AI For Profit</title>
      <description>1. Chapter 1: Introduction to Logitech's plan to use AI for profit
2. Chapter 2: The role of AI in Logitech's marketing strategy
3. Chapter 3: AI in Logitech's product development process</description>
      <pubDate>Mon, 22 May 2023 12:36:14 GMT</pubDate>
      <enclosure url="https://JonathanGrant.github.io/podcasts/audio/logitechs_$100b_plan_to_use_ai_for_profit.mpeg" type="audio/mpeg" length="3676177"/>
    </item>
    <item>
      <title>Logitechs 5 Year Plan to $10B in Revenue by Building on Video Conferencing With AI</title>
      <description>1. Introduction to Logitech's Five-Year Plan: This segment will provide an overview of Logitech's plan to achieve $10B in revenue by leveraging video conferencing and AI technologies.
2. Overview of Logitech's Business Segments: Describe the various business segments that Logitech operates in and how they are contributing to the company's growth.
3. Video Conferencing Market Analysis: Discuss the current state of the video conferencing market, including growth trends, market players, and areas of potential opportunity.
4. Logitech's AI Strategy: Explore Logitech's AI strategy and how it aims to enhance the company's video conferencing capabilities with automated features, virtual assistants, and other intelligent solutions.
5. Collaborative Technologies and Future of Work: Investigate the role of collaborative technologies and the future of work in shaping Logitech's long-term strategy. Reference published scientific journals on the topic.</description>
      <pubDate>Mon, 22 May 2023 13:03:14 GMT</pubDate>
      <enclosure url="https://JonathanGrant.github.io/podcasts/audio/logitechs_5_year_plan_to_$10b_in_revenue_by_building_on_video_conferencing_with_ai.mpeg" type="audio/mpeg" length="6430417"/>
    </item>
    <item>
      <title>Logitechs 5 Year Plan to $10B in Revenue by Building Products in Computer Peripherals, Gaming, and Video Collaboration With AI</title>
      <description>1. Introduction of Logitech's 5-year plan
2. Computer peripherals: expanding beyond the mouse and keyboard
3. Gaming: the rising popularity and potential for growth
4. Video collaboration: integrating AI for a better experience
5. Logitech's sustainability efforts in product development
6. The role of research and development in Logitech's plan
7. Competing with other tech giants in the industry
8. Looking ahead to the future of Logitech and the tech industry in general</description>
      <pubDate>Mon, 22 May 2023 19:40:34 GMT</pubDate>
      <enclosure url="https://JonathanGrant.github.io/podcasts/audio/logitechs_5_year_plan_to_$10b_in_revenue_by_building_products_in_computer_peripherals,_gaming,_and_video_collaboration_with_ai.mpeg" type="audio/mpeg" length="6070845"/>
    </item>
    <item>
      <title>Arxiv Test Title:Q-malizing flow and infinitesimal density ratio estimation</title>
      <description>Title:Q-malizing flow and infinitesimal density ratio estimation</description>
      <pubDate>Wed, 24 May 2023 20:34:59 GMT</pubDate>
      <enclosure url="https://JonathanGrant.github.io/podcasts/audio/arxiv_test_title:q-malizing_flow_and_infinitesimal_density_ratio_estimation.mpeg" type="audio/mpeg" length="4497481"/>
    </item>
    <item>
      <title>Arxiv Test Title:Hedonic Prices and Quality Adjusted Price Indices Powered by AI</title>
      <description>Title:Hedonic Prices and Quality Adjusted Price Indices Powered by AI</description>
      <pubDate>Wed, 24 May 2023 20:50:41 GMT</pubDate>
      <enclosure url="https://JonathanGrant.github.io/podcasts/audio/arxiv_test_title:hedonic_prices_and_quality_adjusted_price_indices_powered_by_ai.mpeg" type="audio/mpeg" length="12732183"/>
    </item>
    <item>
      <title>Arxiv Test Title:Mixture Quantiles Estimated by Constrained Linear Regression</title>
      <description>Title:Mixture Quantiles Estimated by Constrained Linear Regression</description>
      <pubDate>Wed, 24 May 2023 20:54:51 GMT</pubDate>
      <enclosure url="https://JonathanGrant.github.io/podcasts/audio/arxiv_test_title:mixture_quantiles_estimated_by_constrained_linear_regression.mpeg" type="audio/mpeg" length="6751909"/>
    </item>
    <item>
      <title>Arxiv Test Title:LIMA: Less Is More for Alignment</title>
      <description>ChatGPT generated podcast using model='gpt-4' for https://arxiv.org/abs/2305.11206</description>
      <pubDate>Wed, 24 May 2023 21:51:52 GMT</pubDate>
      <enclosure url="https://JonathanGrant.github.io/podcasts/audio/arxiv_test_title:lima:_less_is_more_for_alignment.mpeg" type="audio/mpeg" length="5971562"/>
    </item>
    <item>
      <title>Arxiv Test Title:Model evaluation for extreme risks</title>
      <description>ChatGPT generated podcast using model='gpt-4' for https://arxiv.org/abs/2305.15324</description>
      <pubDate>Wed, 24 May 2023 21:57:01 GMT</pubDate>
      <enclosure url="https://JonathanGrant.github.io/podcasts/audio/arxiv_test_title:model_evaluation_for_extreme_risks.mpeg" type="audio/mpeg" length="9928943"/>
    </item>
    <item>
      <title>Arxiv Test Title:Leveraging Pre-trained Large Language Models to Construct and Utilize World Models for Model-based Task Planning</title>
      <description>ChatGPT generated podcast using model='gpt-4' for https://arxiv.org/abs/2305.14909</description>
      <pubDate>Wed, 24 May 2023 22:05:44 GMT</pubDate>
      <enclosure url="https://JonathanGrant.github.io/podcasts/audio/arxiv_test_title:leveraging_pre-trained_large_language_models_to_construct_and_utilize_world_models_for_model-based_task_planning.mp3" type="audio/mpeg" length="27940799"/>
    </item>
    <item>
      <title>Arxiv Test Title:The False Promise of Imitating Proprietary LLMs</title>
      <description>ChatGPT generated podcast using model='gpt-4' for https://arxiv.org/abs/2305.15717</description>
      <pubDate>Fri, 26 May 2023 07:13:16 GMT</pubDate>
      <enclosure url="https://JonathanGrant.github.io/podcasts/audio/arxiv_test_title:the_false_promise_of_imitating_proprietary_llms.mp3" type="audio/mpeg" length="13354142"/>
    </item>
    <item>
      <title>Arxiv Test Title:Understanding the Capabilities of Large Language Models for Automated Planning</title>
      <description>ChatGPT generated podcast using model='gpt-3.5-turbo' for https://arxiv.org/abs/2305.16151</description>
      <pubDate>Fri, 26 May 2023 10:13:39 GMT</pubDate>
      <enclosure url="https://JonathanGrant.github.io/podcasts/audio/arxiv_test_title:understanding_the_capabilities_of_large_language_models_for_automated_planning.mp3" type="audio/mp3" length="8863680"/>
    </item>
    <item>
      <title>Arxiv Test Title:Judgments of research co-created by generative AI: experimental evidence</title>
      <description>ChatGPT generated podcast using model='gpt-3.5-turbo' for https://arxiv.org/abs/2305.11873</description>
      <pubDate>Fri, 26 May 2023 10:21:59 GMT</pubDate>
      <enclosure url="https://JonathanGrant.github.io/podcasts/audio/arxiv_test_title:judgments_of_research_co-created_by_generative_ai:_experimental_evidence.mp3" type="audio/mp3" length="5699424"/>
    </item>
    <item>
      <title>Arxiv Test Title:Referred by Multi-Modality: A Unified Temporal Transformer for Video Object Segmentation</title>
      <description>ChatGPT generated podcast using model='gpt-3.5-turbo' for https://arxiv.org/abs/2305.16318v1</description>
      <pubDate>Fri, 26 May 2023 10:44:44 GMT</pubDate>
      <enclosure url="https://JonathanGrant.github.io/podcasts/audio/arxiv_test_title:referred_by_multi-modality:_a_unified_temporal_transformer_for_video_object_segmentation.mp3" type="audio/mp3" length="8261088"/>
    </item>
    <item>
      <title>Arxiv Test Title:Parallel Sampling of Diffusion Models</title>
      <description>ChatGPT generated podcast using model='gpt-3.5-turbo' for https://arxiv.org/abs/2305.16317v1</description>
      <pubDate>Fri, 26 May 2023 10:46:43 GMT</pubDate>
      <enclosure url="https://JonathanGrant.github.io/podcasts/audio/arxiv_test_title:parallel_sampling_of_diffusion_models.mp3" type="audio/mp3" length="8073312"/>
    </item>
    <item>
      <title>Arxiv Test Title:UMat: Uncertainty-Aware Single Image High Resolution Material Capture</title>
      <description>ChatGPT generated podcast using model='gpt-3.5-turbo' for https://arxiv.org/abs/2305.16312v1</description>
      <pubDate>Fri, 26 May 2023 10:48:20 GMT</pubDate>
      <enclosure url="https://JonathanGrant.github.io/podcasts/audio/arxiv_test_title:umat:_uncertainty-aware_single_image_high_resolution_material_capture.mp3" type="audio/mp3" length="8931264"/>
    </item>
    <item>
      <title>PDF Test Regularity Normalization: Neuroscience-Inspired Unsupervised Attention across Neural Network Layers</title>
      <description>ChatGPT generated podcast using model='gpt-4' for Regularity Normalization: Neuroscience-Inspired Unsupervised Attention across Neural Network Layers</description>
      <pubDate>Sun, 28 May 2023 14:29:45 GMT</pubDate>
      <enclosure url="https://JonathanGrant.github.io/podcasts/audio/pdf_test_regularity_normalization:_neuroscience-inspired_unsupervised_attention_across_neural_network_layers.mp3" type="audio/mp3" length="15760992"/>
    </item>
    <item>
      <title>Arxiv Test Title:Executive Function: A Contrastive Value Policy for Resampling and Relabeling Perceptions via Hindsight Summarization?</title>
      <description>ChatGPT generated podcast using model='gpt-4' for https://arxiv.org/abs/2204.12639</description>
      <pubDate>Mon, 12 Jun 2023 10:22:26 GMT</pubDate>
      <enclosure url="https://JonathanGrant.github.io/podcasts/audio/arxiv_test_title:executive_function:_a_contrastive_value_policy_for_resampling_and_relabeling_perceptions_via_hindsight_summarization?.mp3" type="audio/mp3" length="5743008"/>
    </item>
    <item>
      <title>Arxiv Test Title:Executive Function: A Contrastive Value Policy for Resampling and Relabeling Perceptions via Hindsight Summarization?</title>
      <description>ChatGPT generated podcast using model='gpt-4' for https://arxiv.org/abs/2204.12639</description>
      <pubDate>Mon, 12 Jun 2023 10:38:51 GMT</pubDate>
      <enclosure url="https://JonathanGrant.github.io/podcasts/audio/arxiv_test_title:executive_function:_a_contrastive_value_policy_for_resampling_and_relabeling_perceptions_via_hindsight_summarization?.mp3" type="audio/mp3" length="5776800"/>
    </item>
    <item>
      <title>Arxiv Test Title Executive Function A Contrastive Value Policy for Resampling and Relabeling Perceptions via Hindsight Summarization </title>
      <description>ChatGPT generated podcast using model='gpt-4' for https://arxiv.org/abs/2204.12639</description>
      <pubDate>Mon, 12 Jun 2023 10:47:15 GMT</pubDate>
      <enclosure url="https://JonathanGrant.github.io/podcasts/audio/arxiv_test_title_executive_function_a_contrastive_value_policy_for_resampling_and_relabeling_perceptions_via_hindsight_summarization_.mp3" type="audio/mp3" length="5776800"/>
    </item>
    <item>
      <title>Arxiv Test Title Mathematical conjecture generation using machine intelligence</title>
      <description>ChatGPT generated podcast using model='gpt-4' for https://arxiv.org/abs/2306.07277</description>
      <pubDate>Tue, 13 Jun 2023 17:58:05 GMT</pubDate>
      <enclosure url="https://JonathanGrant.github.io/podcasts/audio/arxiv_test_title_mathematical_conjecture_generation_using_machine_intelligence.mp3" type="audio/mp3" length="5619360"/>
    </item>
    <item>
      <title>Arxiv Test Title Parameter efficient Dysarthric Speech Recognition Using Adapter Fusion and Householder Transformation</title>
      <description>ChatGPT generated podcast using model='gpt-4' for https://arxiv.org/abs/2306.07090</description>
      <pubDate>Tue, 13 Jun 2023 18:00:28 GMT</pubDate>
      <enclosure url="https://JonathanGrant.github.io/podcasts/audio/arxiv_test_title_parameter_efficient_dysarthric_speech_recognition_using_adapter_fusion_and_householder_transformation.mp3" type="audio/mp3" length="909312"/>
    </item>
  </channel>
</rss>